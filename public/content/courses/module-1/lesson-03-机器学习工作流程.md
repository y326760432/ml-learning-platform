# ç¬¬3è¯¾ï¼šæœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹

## ğŸ“– è¯¾ç¨‹ç›®æ ‡

å­¦å®Œæœ¬è¯¾ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… äº†è§£å®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®æµç¨‹
- âœ… ç†è§£æ•°æ®åœ¨æ•´ä¸ªæµç¨‹ä¸­çš„é‡è¦æ€§
- âœ… æŒæ¡æ•°æ®é¢„å¤„ç†çš„åŸºæœ¬æ–¹æ³•
- âœ… äº†è§£æ¨¡å‹è¯„ä¼°å’Œéƒ¨ç½²çš„è¦ç‚¹

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š45-50åˆ†é’Ÿ  
**éš¾åº¦ç­‰çº§**ï¼šâ­â­ è¿›é˜¶

---

## ğŸ”„ æœºå™¨å­¦ä¹ é¡¹ç›®çš„å®Œæ•´æµç¨‹

ä¸€ä¸ªå®Œæ•´çš„æœºå™¨å­¦ä¹ é¡¹ç›®é€šå¸¸åŒ…å«ä»¥ä¸‹å‡ ä¸ªé˜¶æ®µï¼š

```mermaid
graph LR
    A[1ï¸âƒ£ é—®é¢˜å®šä¹‰] --> B[2ï¸âƒ£ æ•°æ®æ”¶é›†]
    B --> C[3ï¸âƒ£ æ•°æ®æ¢ç´¢]
    C --> D[4ï¸âƒ£ æ•°æ®é¢„å¤„ç†]
    D --> E[5ï¸âƒ£ ç‰¹å¾å·¥ç¨‹]
    E --> F[6ï¸âƒ£ æ¨¡å‹é€‰æ‹©]
    F --> G[7ï¸âƒ£ æ¨¡å‹è®­ç»ƒ]
    G --> H[8ï¸âƒ£ æ¨¡å‹è¯„ä¼°]
    H --> I{æ•ˆæœæ»¡æ„?}
    I -->|å¦| E
    I -->|æ˜¯| J[9ï¸âƒ£ æ¨¡å‹éƒ¨ç½²]
    J --> K[ğŸ”Ÿ ç›‘æ§ç»´æŠ¤]
    
    style A fill:#FFE5E5,stroke:#FF6B6B,stroke-width:3px
    style B fill:#FFF4E5,stroke:#FFA726,stroke-width:3px
    style C fill:#FFF9E5,stroke:#FFC107,stroke-width:3px
    style D fill:#F0F8FF,stroke:#42A5F5,stroke-width:3px
    style E fill:#E8F5E9,stroke:#66BB6A,stroke-width:3px
    style F fill:#F3E5F5,stroke:#AB47BC,stroke-width:3px
    style G fill:#E1F5FE,stroke:#29B6F6,stroke-width:3px
    style H fill:#FFF3E0,stroke:#FF9800,stroke-width:3px
    style I fill:#FFEBEE,stroke:#EF5350,stroke-width:3px
    style J fill:#E8EAF6,stroke:#5C6BC0,stroke-width:3px
    style K fill:#E0F2F1,stroke:#26A69A,stroke-width:3px
```

---

## 1ï¸âƒ£ é—®é¢˜å®šä¹‰ï¼ˆProblem Definitionï¼‰

### æ ¸å¿ƒé—®é¢˜

**åœ¨å¼€å§‹ä»»ä½•å·¥ä½œä¹‹å‰ï¼Œå¿…é¡»æ˜ç¡®ä»¥ä¸‹é—®é¢˜ï¼š**

1. **æˆ‘ä»¬è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ**
2. **è¿™ä¸ªé—®é¢˜èƒ½ç”¨æœºå™¨å­¦ä¹ è§£å†³å—ï¼Ÿ**
3. **è§£å†³è¿™ä¸ªé—®é¢˜çš„ä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ**
4. **æˆ‘ä»¬æœ‰ä»€ä¹ˆæ ·çš„æ•°æ®ï¼Ÿ**
5. **å¦‚ä½•è¡¡é‡æˆåŠŸï¼Ÿ**

### ç¤ºä¾‹ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹é¡¹ç›®

| é—®é¢˜ç»´åº¦ | å…·ä½“å†…å®¹ |
|---------|---------|
| **é—®é¢˜æè¿°** | è‡ªåŠ¨è¯†åˆ«å¹¶è¿‡æ»¤åƒåœ¾é‚®ä»¶ |
| **é—®é¢˜ç±»å‹** | ç›‘ç£å­¦ä¹  - äºŒåˆ†ç±»é—®é¢˜ |
| **è¾“å…¥æ•°æ®** | é‚®ä»¶æ–‡æœ¬ã€å‘ä»¶äººã€ä¸»é¢˜ç­‰ |
| **è¾“å‡ºç»“æœ** | åƒåœ¾é‚®ä»¶ / æ­£å¸¸é‚®ä»¶ |
| **æˆåŠŸæŒ‡æ ‡** | å‡†ç¡®ç‡ > 95%ï¼Œè¯¯æŠ¥ç‡ < 1% |
| **ä¸šåŠ¡ä»·å€¼** | æå‡ç”¨æˆ·ä½“éªŒï¼Œå‡å°‘ç”¨æˆ·æŸå¤± |

### ğŸ“ é—®é¢˜å®šä¹‰æ¸…å•

åœ¨å¼€å§‹é¡¹ç›®å‰ï¼Œç¡®ä¿å›ç­”äº†ä»¥ä¸‹é—®é¢˜ï¼š

```
â˜‘ é—®é¢˜æ˜¯å¦æ¸…æ™°å®šä¹‰ï¼Ÿ
â˜‘ æ˜¯å¦éœ€è¦æœºå™¨å­¦ä¹ ï¼Ÿï¼ˆç®€å•è§„åˆ™æ˜¯å¦è¶³å¤Ÿï¼Ÿï¼‰
â˜‘ æœ‰è¶³å¤Ÿçš„æ•°æ®å—ï¼Ÿ
â˜‘ æ•°æ®è´¨é‡å¦‚ä½•ï¼Ÿ
â˜‘ å¦‚ä½•è¡¡é‡æ¨¡å‹æ€§èƒ½ï¼Ÿ
â˜‘ é¢„æœŸçš„æŠ•å…¥äº§å‡ºæ¯”æ˜¯å¤šå°‘ï¼Ÿ
â˜‘ æœ‰å“ªäº›çº¦æŸæ¡ä»¶ï¼Ÿï¼ˆæ—¶é—´ã€æˆæœ¬ã€éšç§ï¼‰
```

---

## 2ï¸âƒ£ æ•°æ®æ”¶é›†ï¼ˆData Collectionï¼‰

### æ•°æ®æ¥æº

**å†…éƒ¨æ•°æ®æº**ï¼š
- ğŸ—„ï¸ å…¬å¸æ•°æ®åº“
- ğŸ“Š ä¸šåŠ¡æ—¥å¿—
- ğŸ“ ç”¨æˆ·è¡Œä¸ºæ•°æ®
- ğŸ’¾ ä¼ æ„Ÿå™¨æ•°æ®

**å¤–éƒ¨æ•°æ®æº**ï¼š
- ğŸŒ å…¬å¼€æ•°æ®é›†ï¼ˆKaggleã€UCIã€GitHubï¼‰
- ğŸ”Œ APIæ¥å£ï¼ˆå¤©æ°”ã€è‚¡ç¥¨ã€ç¤¾äº¤åª’ä½“ï¼‰
- ğŸ•·ï¸ ç½‘ç»œçˆ¬è™«
- ğŸ’° è´­ä¹°æ•°æ®

### æ•°æ®æ”¶é›†è¦ç‚¹

#### æ•°æ®é‡è¦æ±‚

| é—®é¢˜ç±»å‹ | æ¨èæ•°æ®é‡ | è¯´æ˜ |
|---------|-----------|------|
| ç®€å•åˆ†ç±» | 1,000+ æ ·æœ¬ | æ¯ä¸ªç±»åˆ«è‡³å°‘å‡ ç™¾ä¸ª |
| å¤æ‚åˆ†ç±» | 10,000+ æ ·æœ¬ | ç±»åˆ«è¶Šå¤šï¼Œéœ€è¦è¶Šå¤šæ•°æ® |
| å›¾åƒè¯†åˆ« | 10,000+ å›¾ç‰‡ | æ·±åº¦å­¦ä¹ éœ€è¦æ›´å¤š |
| æ–‡æœ¬åˆ†ç±» | 5,000+ æ–‡æ¡£ | ä¾èµ–ä»»åŠ¡å¤æ‚åº¦ |
| å›å½’é—®é¢˜ | 1,000+ æ ·æœ¬ | ç‰¹å¾è¶Šå¤šï¼Œéœ€è¦è¶Šå¤š |

#### æ•°æ®è´¨é‡è¦æ±‚

âœ… **å®Œæ•´æ€§**ï¼šæ•°æ®æ˜¯å¦æœ‰ç¼ºå¤±å€¼ï¼Ÿ  
âœ… **å‡†ç¡®æ€§**ï¼šæ•°æ®æ˜¯å¦æ­£ç¡®ï¼Ÿæœ‰æ— é”™è¯¯æ ‡æ³¨ï¼Ÿ  
âœ… **ä¸€è‡´æ€§**ï¼šæ ¼å¼æ˜¯å¦ç»Ÿä¸€ï¼Ÿ  
âœ… **æ—¶æ•ˆæ€§**ï¼šæ•°æ®æ˜¯å¦è¿‡æ—¶ï¼Ÿ  
âœ… **ç›¸å…³æ€§**ï¼šæ•°æ®æ˜¯å¦ä¸é—®é¢˜ç›¸å…³ï¼Ÿ

### ä»£ç ç¤ºä¾‹ï¼šåŠ è½½æ•°æ®

```python
import pandas as pd
import numpy as np

# ä»CSVæ–‡ä»¶åŠ è½½æ•°æ®
data = pd.read_csv('data/emails.csv')

# æŸ¥çœ‹æ•°æ®åŸºæœ¬ä¿¡æ¯
print(f"æ•°æ®å½¢çŠ¶: {data.shape}")  # (è¡Œæ•°, åˆ—æ•°)
print(f"\nå‰5è¡Œæ•°æ®:")
print(data.head())

# æ•°æ®åŸºæœ¬ç»Ÿè®¡
print(f"\næ•°æ®ç»Ÿè®¡ä¿¡æ¯:")
print(data.describe())

# æ£€æŸ¥æ•°æ®ç±»å‹
print(f"\næ•°æ®ç±»å‹:")
print(data.dtypes)

# æ£€æŸ¥ç¼ºå¤±å€¼
print(f"\nç¼ºå¤±å€¼ç»Ÿè®¡:")
print(data.isnull().sum())
```

---

## 3ï¸âƒ£ æ•°æ®æ¢ç´¢ï¼ˆData Exploration/EDAï¼‰

### ä»€ä¹ˆæ˜¯æ•°æ®æ¢ç´¢ï¼Ÿ

**æ¢ç´¢æ€§æ•°æ®åˆ†æï¼ˆEDAï¼‰** æ˜¯åœ¨å»ºæ¨¡å‰äº†è§£æ•°æ®çš„è¿‡ç¨‹ï¼Œç›®çš„æ˜¯ï¼š
- ğŸ“Š äº†è§£æ•°æ®åˆ†å¸ƒ
- ğŸ” å‘ç°æ•°æ®æ¨¡å¼
- ğŸ› æ£€æµ‹å¼‚å¸¸å€¼
- ğŸ”— å‘ç°å˜é‡é—´å…³ç³»

### æ•°æ®æ¢ç´¢çš„æ­¥éª¤

#### æ­¥éª¤1ï¼šæŸ¥çœ‹æ•°æ®æ¦‚è§ˆ

```python
# æ•°æ®åŸºæœ¬ä¿¡æ¯
print("=" * 50)
print("æ•°æ®é›†æ¦‚è§ˆ")
print("=" * 50)

# æ ·æœ¬æ•°å’Œç‰¹å¾æ•°
print(f"æ ·æœ¬æ•°: {data.shape[0]}")
print(f"ç‰¹å¾æ•°: {data.shape[1]}")

# ç‰¹å¾åç§°
print(f"\nç‰¹å¾åˆ—è¡¨:")
for i, col in enumerate(data.columns, 1):
    print(f"  {i}. {col} ({data[col].dtype})")
```

#### æ­¥éª¤2ï¼šç»Ÿè®¡åˆ†æ

```python
# æ•°å€¼å‹ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
numerical_cols = data.select_dtypes(include=[np.number]).columns
print("\næ•°å€¼å‹ç‰¹å¾ç»Ÿè®¡:")
print(data[numerical_cols].describe())

# ç±»åˆ«å‹ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯
categorical_cols = data.select_dtypes(include=['object']).columns
print("\nç±»åˆ«å‹ç‰¹å¾ç»Ÿè®¡:")
for col in categorical_cols:
    print(f"\n{col}:")
    print(data[col].value_counts().head())
```

#### æ­¥éª¤3ï¼šå¯è§†åŒ–åˆ†æ

```python
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®æ ·å¼
sns.set_style("whitegrid")
plt.rcParams['font.sans-serif'] = ['SimHei']  # ä¸­æ–‡æ”¯æŒ

# 1. ç›®æ ‡å˜é‡åˆ†å¸ƒ
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
data['label'].value_counts().plot(kind='bar', color=['#FF6B6B', '#4ECDC4'])
plt.title('ç›®æ ‡å˜é‡åˆ†å¸ƒ')
plt.xlabel('ç±»åˆ«')
plt.ylabel('æ•°é‡')

# 2. æ•°å€¼ç‰¹å¾åˆ†å¸ƒ
plt.subplot(1, 2, 2)
data['feature'].hist(bins=30, edgecolor='black')
plt.title('ç‰¹å¾åˆ†å¸ƒ')
plt.xlabel('ç‰¹å¾å€¼')
plt.ylabel('é¢‘æ•°')

plt.tight_layout()
plt.show()

# 3. ç›¸å…³æ€§çŸ©é˜µ
plt.figure(figsize=(10, 8))
correlation = data[numerical_cols].corr()
sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0)
plt.title('ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ')
plt.show()

# 4. ç®±çº¿å›¾ï¼ˆæ£€æµ‹å¼‚å¸¸å€¼ï¼‰
plt.figure(figsize=(12, 6))
data[numerical_cols].boxplot()
plt.title('ç®±çº¿å›¾ - å¼‚å¸¸å€¼æ£€æµ‹')
plt.xticks(rotation=45)
plt.show()
```

### ğŸ“Š æ•°æ®æ¢ç´¢å‘ç°æ¸…å•

æ¢ç´¢æ•°æ®æ—¶ï¼Œå…³æ³¨ä»¥ä¸‹é—®é¢˜ï¼š

```
ğŸ“ˆ æ•°æ®åˆ†å¸ƒ
  â˜‘ ç›®æ ‡å˜é‡æ˜¯å¦å¹³è¡¡ï¼Ÿ
  â˜‘ ç‰¹å¾å€¼çš„èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿ
  â˜‘ æ˜¯å¦å­˜åœ¨æ˜æ˜¾çš„åæ€åˆ†å¸ƒï¼Ÿ

ğŸ› æ•°æ®è´¨é‡
  â˜‘ æœ‰å¤šå°‘ç¼ºå¤±å€¼ï¼Ÿ
  â˜‘ æœ‰å¼‚å¸¸å€¼å—ï¼Ÿ
  â˜‘ æ•°æ®æ˜¯å¦ä¸€è‡´ï¼Ÿ

ğŸ”— ç‰¹å¾å…³ç³»
  â˜‘ ç‰¹å¾ä¹‹é—´æ˜¯å¦é«˜åº¦ç›¸å…³ï¼Ÿ
  â˜‘ å“ªäº›ç‰¹å¾ä¸ç›®æ ‡å˜é‡æœ€ç›¸å…³ï¼Ÿ
  â˜‘ æ˜¯å¦å­˜åœ¨éçº¿æ€§å…³ç³»ï¼Ÿ

ğŸ’¡ ä¸šåŠ¡ç†è§£
  â˜‘ æ•°æ®æ˜¯å¦ç¬¦åˆä¸šåŠ¡é€»è¾‘ï¼Ÿ
  â˜‘ æ˜¯å¦éœ€è¦é¢†åŸŸä¸“å®¶çš„å¸®åŠ©ï¼Ÿ
```

---

## 4ï¸âƒ£ æ•°æ®é¢„å¤„ç†ï¼ˆData Preprocessingï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®é¢„å¤„ç†ï¼Ÿ

**"Garbage In, Garbage Out"** - åƒåœ¾æ•°æ®åªèƒ½äº§ç”Ÿåƒåœ¾æ¨¡å‹

æ•°æ®é¢„å¤„ç†æ˜¯æœºå™¨å­¦ä¹ ä¸­**æœ€è€—æ—¶ä½†æœ€é‡è¦**çš„æ­¥éª¤ï¼Œé€šå¸¸å æ•´ä¸ªé¡¹ç›®æ—¶é—´çš„ **60-80%**ã€‚

### æ•°æ®é¢„å¤„ç†çš„ä¸»è¦ä»»åŠ¡

#### 1. å¤„ç†ç¼ºå¤±å€¼

**æ£€æµ‹ç¼ºå¤±å€¼**ï¼š

```python
# æŸ¥çœ‹ç¼ºå¤±å€¼
missing = data.isnull().sum()
missing_percent = 100 * missing / len(data)

missing_df = pd.DataFrame({
    'ç¼ºå¤±æ•°é‡': missing,
    'ç¼ºå¤±ç™¾åˆ†æ¯”': missing_percent
})

print(missing_df[missing_df['ç¼ºå¤±æ•°é‡'] > 0].sort_values('ç¼ºå¤±æ•°é‡', ascending=False))
```

**å¤„ç†ç­–ç•¥**ï¼š

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | ä»£ç ç¤ºä¾‹ |
|------|---------|---------|
| **åˆ é™¤** | ç¼ºå¤±æ¯”ä¾‹ < 5% | `data.dropna()` |
| **å‡å€¼å¡«å……** | æ•°å€¼å‹ï¼Œæ­£æ€åˆ†å¸ƒ | `data.fillna(data.mean())` |
| **ä¸­ä½æ•°å¡«å……** | æ•°å€¼å‹ï¼Œæœ‰å¼‚å¸¸å€¼ | `data.fillna(data.median())` |
| **ä¼—æ•°å¡«å……** | ç±»åˆ«å‹ç‰¹å¾ | `data.fillna(data.mode()[0])` |
| **å‰å‘/åå‘å¡«å……** | æ—¶é—´åºåˆ—æ•°æ® | `data.fillna(method='ffill')` |
| **é¢„æµ‹å¡«å……** | é‡è¦ç‰¹å¾ï¼Œç¼ºå¤±å°‘ | ç”¨æ¨¡å‹é¢„æµ‹ç¼ºå¤±å€¼ |

#### 2. å¤„ç†å¼‚å¸¸å€¼

**æ£€æµ‹æ–¹æ³•**ï¼š

```python
# æ–¹æ³•1ï¼šç»Ÿè®¡æ–¹æ³•ï¼ˆ3å€æ ‡å‡†å·®ï¼‰
def detect_outliers_zscore(data, column, threshold=3):
    mean = data[column].mean()
    std = data[column].std()
    z_scores = (data[column] - mean) / std
    return data[abs(z_scores) > threshold]

# æ–¹æ³•2ï¼šIQRæ–¹æ³•ï¼ˆå››åˆ†ä½è·ï¼‰
def detect_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] < lower_bound) | (data[column] > upper_bound)]

# ä½¿ç”¨ç¤ºä¾‹
outliers = detect_outliers_iqr(data, 'price')
print(f"æ£€æµ‹åˆ° {len(outliers)} ä¸ªå¼‚å¸¸å€¼")
```

**å¤„ç†ç­–ç•¥**ï¼š

- **åˆ é™¤**ï¼šç¡®è®¤æ˜¯é”™è¯¯æ•°æ®ååˆ é™¤
- **æ›¿æ¢**ï¼šç”¨å‡å€¼ã€ä¸­ä½æ•°æˆ–è¾¹ç•Œå€¼æ›¿æ¢
- **ä¿ç•™**ï¼šå¦‚æœæ˜¯çœŸå®æç«¯å€¼ï¼Œä¿ç•™å¹¶æ ‡è®°
- **å•ç‹¬å»ºæ¨¡**ï¼šä¸ºå¼‚å¸¸å€¼å»ºç«‹å•ç‹¬çš„æ¨¡å‹

#### 3. ç‰¹å¾ç¼–ç 

**ç±»åˆ«å‹ç‰¹å¾è½¬æ•°å€¼**ï¼š

```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# æ–¹æ³•1ï¼šæ ‡ç­¾ç¼–ç ï¼ˆæœ‰åºç±»åˆ«ï¼‰
# é€‚ç”¨äºï¼šæ•™è‚²ç¨‹åº¦ï¼ˆå°å­¦<åˆä¸­<é«˜ä¸­<å¤§å­¦ï¼‰
le = LabelEncoder()
data['education_encoded'] = le.fit_transform(data['education'])

# æ–¹æ³•2ï¼šç‹¬çƒ­ç¼–ç ï¼ˆæ— åºç±»åˆ«ï¼‰
# é€‚ç”¨äºï¼šé¢œè‰²ï¼ˆçº¢ã€è“ã€ç»¿ï¼‰
data_encoded = pd.get_dummies(data, columns=['color'], prefix='color')

# æ–¹æ³•3ï¼šç›®æ ‡ç¼–ç ï¼ˆç±»åˆ«å¾ˆå¤šæ—¶ï¼‰
# ç”¨ç›®æ ‡å˜é‡çš„å‡å€¼æ›¿æ¢ç±»åˆ«
category_means = data.groupby('city')['target'].mean()
data['city_encoded'] = data['city'].map(category_means)
```

#### 4. ç‰¹å¾ç¼©æ”¾

**ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾ï¼Ÿ**

ä¸åŒç‰¹å¾çš„é‡çº²å¯èƒ½å·®å¼‚å·¨å¤§ï¼š
- å¹´é¾„ï¼š18-65
- æ”¶å…¥ï¼š30,000-500,000

å¯¹æŸäº›ç®—æ³•ï¼ˆKNNã€SVMã€ç¥ç»ç½‘ç»œï¼‰å½±å“å¾ˆå¤§ã€‚

**ç¼©æ”¾æ–¹æ³•**ï¼š

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# æ–¹æ³•1ï¼šæ ‡å‡†åŒ–ï¼ˆZ-score normalizationï¼‰
# å°†æ•°æ®è½¬æ¢ä¸ºå‡å€¼0ï¼Œæ ‡å‡†å·®1
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data[numerical_cols])

# æ–¹æ³•2ï¼šå½’ä¸€åŒ–ï¼ˆMin-Max scalingï¼‰
# å°†æ•°æ®ç¼©æ”¾åˆ°[0, 1]åŒºé—´
scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data[numerical_cols])

# æ–¹æ³•3ï¼šé²æ£’ç¼©æ”¾ï¼ˆRobust scalingï¼‰
# å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
scaler = RobustScaler()
data_scaled = scaler.fit_transform(data[numerical_cols])
```

**é€‰æ‹©å»ºè®®**ï¼š

| ç®—æ³•ç±»å‹ | æ¨èæ–¹æ³• | åŸå›  |
|---------|---------|------|
| çº¿æ€§å›å½’ã€é€»è¾‘å›å½’ | æ ‡å‡†åŒ– | æ”¶æ•›æ›´å¿« |
| ç¥ç»ç½‘ç»œ | æ ‡å‡†åŒ– | æ¢¯åº¦ä¸‹é™æ•ˆæœå¥½ |
| SVMã€KNN | å½’ä¸€åŒ–æˆ–æ ‡å‡†åŒ– | è·ç¦»è®¡ç®—æ•æ„Ÿ |
| å†³ç­–æ ‘ã€éšæœºæ£®æ— | ä¸éœ€è¦ | åŸºäºè§„åˆ™ï¼Œä¸å—å°ºåº¦å½±å“ |
| æœ‰å¼‚å¸¸å€¼ | é²æ£’ç¼©æ”¾ | å‡å°‘å¼‚å¸¸å€¼å½±å“ |

#### 5. æ•°æ®åˆ’åˆ†

```python
from sklearn.model_selection import train_test_split

# åˆ’åˆ†ç‰¹å¾å’Œç›®æ ‡
X = data.drop('target', axis=1)
y = data['target']

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ80% / 20%ï¼‰
X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2,      # æµ‹è¯•é›†æ¯”ä¾‹
    random_state=42,    # éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
    stratify=y          # åˆ†å±‚é‡‡æ ·ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹
)

print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape}")
print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape}")
```

### å®Œæ•´çš„é¢„å¤„ç†æµç¨‹

```python
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer

# å®šä¹‰æ•°å€¼å‹ç‰¹å¾çš„å¤„ç†æµç¨‹
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),  # å¡«å……ç¼ºå¤±å€¼
    ('scaler', StandardScaler())                     # æ ‡å‡†åŒ–
])

# å®šä¹‰ç±»åˆ«å‹ç‰¹å¾çš„å¤„ç†æµç¨‹
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # å¡«å……ç¼ºå¤±å€¼
    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # ç‹¬çƒ­ç¼–ç 
])

# ç»„åˆæ‰€æœ‰é¢„å¤„ç†æ­¥éª¤
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# åº”ç”¨é¢„å¤„ç†
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)
```

---

## 5ï¸âƒ£ ç‰¹å¾å·¥ç¨‹ï¼ˆFeature Engineeringï¼‰

### ä»€ä¹ˆæ˜¯ç‰¹å¾å·¥ç¨‹ï¼Ÿ

> "ç‰¹å¾å·¥ç¨‹æ˜¯æœºå™¨å­¦ä¹ çš„å…³é”®ï¼Œæ¯”ç®—æ³•é€‰æ‹©æ›´é‡è¦" - Andrew Ng

**ç‰¹å¾å·¥ç¨‹**ï¼šä»åŸå§‹æ•°æ®ä¸­åˆ›é€ æ–°ç‰¹å¾ï¼Œæå‡æ¨¡å‹æ€§èƒ½ã€‚

### å¸¸ç”¨ç‰¹å¾å·¥ç¨‹æŠ€å·§

#### 1. ç‰¹å¾åˆ›å»º

**ç»„åˆç‰¹å¾**ï¼š

```python
# ç¤ºä¾‹ï¼šæˆ¿ä»·é¢„æµ‹
# åˆ›å»ºæ–°ç‰¹å¾ï¼šæ€»é¢ç§¯ = å§å®¤é¢ç§¯ + å®¢å…é¢ç§¯ + å¨æˆ¿é¢ç§¯
data['total_area'] = data['bedroom_area'] + data['living_area'] + data['kitchen_area']

# åˆ›å»ºæ¯”ä¾‹ç‰¹å¾
data['bedroom_ratio'] = data['bedroom_area'] / data['total_area']

# åˆ›å»ºäº¤äº’ç‰¹å¾
data['area_x_quality'] = data['area'] * data['quality_score']
```

**æ—¶é—´ç‰¹å¾æå–**ï¼š

```python
# ä»æ—¥æœŸæå–ç‰¹å¾
data['year'] = data['date'].dt.year
data['month'] = data['date'].dt.month
data['day_of_week'] = data['date'].dt.dayofweek
data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)
data['quarter'] = data['date'].dt.quarter
```

**æ–‡æœ¬ç‰¹å¾**ï¼š

```python
# æ–‡æœ¬é•¿åº¦
data['text_length'] = data['text'].str.len()

# è¯æ•°
data['word_count'] = data['text'].str.split().str.len()

# å¤§å†™å­—æ¯æ¯”ä¾‹
data['upper_ratio'] = data['text'].str.count(r'[A-Z]') / data['text_length']
```

#### 2. ç‰¹å¾é€‰æ‹©

**ä¸ºä»€ä¹ˆéœ€è¦ç‰¹å¾é€‰æ‹©ï¼Ÿ**

- âš¡ **æå‡æ€§èƒ½**ï¼šå‡å°‘å™ªéŸ³ç‰¹å¾
- ğŸš€ **åŠ å¿«è®­ç»ƒ**ï¼šå‡å°‘è®¡ç®—é‡
- ğŸ’¡ **æé«˜å¯è§£é‡Šæ€§**ï¼šæ›´å®¹æ˜“ç†è§£æ¨¡å‹

**æ–¹æ³•1ï¼šè¿‡æ»¤æ³•ï¼ˆFilterï¼‰**

```python
from sklearn.feature_selection import SelectKBest, chi2, f_classif

# é€‰æ‹©Kä¸ªæœ€ä½³ç‰¹å¾ï¼ˆåŸºäºå¡æ–¹æ£€éªŒï¼‰
selector = SelectKBest(score_func=chi2, k=10)
X_selected = selector.fit_transform(X, y)

# æŸ¥çœ‹è¢«é€‰ä¸­çš„ç‰¹å¾
selected_features = X.columns[selector.get_support()]
print(f"é€‰ä¸­çš„ç‰¹å¾: {list(selected_features)}")
```

**æ–¹æ³•2ï¼šåŒ…è£¹æ³•ï¼ˆWrapperï¼‰**

```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# é€’å½’ç‰¹å¾æ¶ˆé™¤
model = RandomForestClassifier(n_estimators=100)
rfe = RFE(estimator=model, n_features_to_select=10)
rfe.fit(X, y)

# æŸ¥çœ‹ç‰¹å¾æ’å
ranking_df = pd.DataFrame({
    'Feature': X.columns,
    'Ranking': rfe.ranking_
}).sort_values('Ranking')

print(ranking_df)
```

**æ–¹æ³•3ï¼šåµŒå…¥æ³•ï¼ˆEmbeddedï¼‰**

```python
from sklearn.ensemble import RandomForestClassifier

# åŸºäºæ ‘æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# è·å–ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

# å¯è§†åŒ–
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['Feature'][:10], feature_importance['Importance'][:10])
plt.xlabel('é‡è¦æ€§')
plt.title('Top 10 é‡è¦ç‰¹å¾')
plt.gca().invert_yaxis()
plt.show()
```

---

## 6ï¸âƒ£ æ¨¡å‹é€‰æ‹©ï¼ˆModel Selectionï¼‰

### å¦‚ä½•é€‰æ‹©æ¨¡å‹ï¼Ÿ

é€‰æ‹©æ¨¡å‹æ—¶è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š

| è€ƒè™‘å› ç´  | é—®é¢˜ |
|---------|------|
| **é—®é¢˜ç±»å‹** | åˆ†ç±»ï¼Ÿå›å½’ï¼Ÿèšç±»ï¼Ÿ |
| **æ•°æ®é‡** | æ ·æœ¬æ•°å¤Ÿå¤šå—ï¼Ÿ |
| **æ•°æ®ç»´åº¦** | ç‰¹å¾æ•°å¤šå—ï¼Ÿ |
| **æ•°æ®çº¿æ€§æ€§** | çº¿æ€§å¯åˆ†å—ï¼Ÿ |
| **è®­ç»ƒæ—¶é—´** | æœ‰æ—¶é—´é™åˆ¶å—ï¼Ÿ |
| **å¯è§£é‡Šæ€§** | éœ€è¦è§£é‡Šæ¨¡å‹å—ï¼Ÿ |
| **å‡†ç¡®æ€§è¦æ±‚** | å¯¹æ€§èƒ½è¦æ±‚é«˜å—ï¼Ÿ |

### å¸¸ç”¨ç®—æ³•é€‰æ‹©æŒ‡å—

```mermaid
graph TD
    Start[é€‰æ‹©ç®—æ³•] --> Q1{é—®é¢˜ç±»å‹?}
    Q1 -->|åˆ†ç±»| Q2{æ ·æœ¬æ•°?}
    Q1 -->|å›å½’| Q5{ç‰¹å¾æ•°?}
    Q1 -->|èšç±»| K[K-Means/å±‚æ¬¡èšç±»]
    
    Q2 -->|< 100k| Q3{çº¿æ€§å¯åˆ†?}
    Q2 -->|> 100k| SGD[SGDåˆ†ç±»å™¨]
    
    Q3 -->|æ˜¯| LR[é€»è¾‘å›å½’/çº¿æ€§SVM]
    Q3 -->|å¦| Q4{éœ€è¦è§£é‡Šæ€§?}
    
    Q4 -->|æ˜¯| DT[å†³ç­–æ ‘/éšæœºæ£®æ—]
    Q4 -->|å¦| SVM[æ ¸SVM/ç¥ç»ç½‘ç»œ]
    
    Q5 -->|< 100| LReg[çº¿æ€§å›å½’]
    Q5 -->|> 100| Q6{çº¿æ€§å…³ç³»?}
    
    Q6 -->|æ˜¯| Ridge[å²­å›å½’/Lasso]
    Q6 -->|å¦| RF[éšæœºæ£®æ—/XGBoost]
    
    style Start fill:#FFE5E5
    style LR fill:#E8F5E9
    style DT fill:#E8F5E9
    style SVM fill:#E8F5E9
    style LReg fill:#E8F5E9
    style RF fill:#E8F5E9
```

### ä»ç®€å•åˆ°å¤æ‚

**æ¨èç­–ç•¥**ï¼šAlways start simple!

1. **åŸºçº¿æ¨¡å‹**ï¼šæœ€ç®€å•çš„æ¨¡å‹ï¼ˆå¦‚é€»è¾‘å›å½’ï¼‰
2. **æ”¹è¿›æ¨¡å‹**ï¼šå°è¯•æ›´å¤æ‚çš„æ¨¡å‹
3. **é›†æˆæ¨¡å‹**ï¼šç»„åˆå¤šä¸ªæ¨¡å‹

```python
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

# åˆ›å»ºå¤šä¸ªæ¨¡å‹
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(n_estimators=100),
    'Gradient Boosting': GradientBoostingClassifier(),
    'SVM': SVC()
}

# è®­ç»ƒå¹¶è¯„ä¼°æ¯ä¸ªæ¨¡å‹
for name, model in models.items():
    model.fit(X_train, y_train)
    score = model.score(X_test, y_test)
    print(f"{name}: {score:.4f}")
```

---

## 7ï¸âƒ£ æ¨¡å‹è®­ç»ƒï¼ˆModel Trainingï¼‰

### è®­ç»ƒè¿‡ç¨‹

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# 1. åˆ›å»ºæ¨¡å‹
model = RandomForestClassifier(
    n_estimators=100,      # æ ‘çš„æ•°é‡
    max_depth=10,          # æ ‘çš„æœ€å¤§æ·±åº¦
    min_samples_split=5,   # åˆ†è£‚æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
    random_state=42
)

# 2. è®­ç»ƒæ¨¡å‹
print("å¼€å§‹è®­ç»ƒ...")
model.fit(X_train, y_train)
print("è®­ç»ƒå®Œæˆï¼")

# 3. äº¤å‰éªŒè¯
cv_scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"\n5æŠ˜äº¤å‰éªŒè¯å‡†ç¡®ç‡: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
```

### è¶…å‚æ•°è°ƒä¼˜

**æ–¹æ³•1ï¼šç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰**

```python
from sklearn.model_selection import GridSearchCV

# å®šä¹‰è¶…å‚æ•°ç½‘æ ¼
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# ç½‘æ ¼æœç´¢
grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_grid=param_grid,
    cv=5,                    # 5æŠ˜äº¤å‰éªŒè¯
    scoring='accuracy',      # è¯„ä¼°æŒ‡æ ‡
    n_jobs=-1,              # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
    verbose=2               # æ˜¾ç¤ºè¿›åº¦
)

# æ‰§è¡Œæœç´¢
grid_search.fit(X_train, y_train)

# æœ€ä½³å‚æ•°
print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {grid_search.best_score_:.4f}")

# ä½¿ç”¨æœ€ä½³æ¨¡å‹
best_model = grid_search.best_estimator_
```

**æ–¹æ³•2ï¼šéšæœºæœç´¢ï¼ˆRandom Searchï¼‰**

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# å®šä¹‰å‚æ•°åˆ†å¸ƒ
param_distributions = {
    'n_estimators': randint(50, 300),
    'max_depth': randint(5, 20),
    'min_samples_split': randint(2, 20),
    'min_samples_leaf': randint(1, 10)
}

# éšæœºæœç´¢
random_search = RandomizedSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,              # å°è¯•50ç§ç»„åˆ
    cv=5,
    scoring='accuracy',
    random_state=42,
    n_jobs=-1,
    verbose=2
)

random_search.fit(X_train, y_train)

print(f"æœ€ä½³å‚æ•°: {random_search.best_params_}")
print(f"æœ€ä½³å¾—åˆ†: {random_search.best_score_:.4f}")
```

---

## 8ï¸âƒ£ æ¨¡å‹è¯„ä¼°ï¼ˆModel Evaluationï¼‰

### åˆ†ç±»æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

#### 1. æ··æ·†çŸ©é˜µ

```python
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# é¢„æµ‹
y_pred = model.predict(X_test)

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_test, y_pred)

# å¯è§†åŒ–
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('æ··æ·†çŸ©é˜µ')
plt.ylabel('çœŸå®æ ‡ç­¾')
plt.xlabel('é¢„æµ‹æ ‡ç­¾')
plt.show()

# è¯¦ç»†æŠ¥å‘Š
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(y_test, y_pred))
```

#### 2. è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | å…¬å¼ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| **å‡†ç¡®ç‡** | (TP+TN)/(TP+TN+FP+FN) | ç±»åˆ«å¹³è¡¡æ—¶ |
| **ç²¾ç¡®ç‡** | TP/(TP+FP) | å…³æ³¨è¯¯æŠ¥ç‡ |
| **å¬å›ç‡** | TP/(TP+FN) | å…³æ³¨æ¼æŠ¥ç‡ |
| **F1åˆ†æ•°** | 2Ã—ç²¾ç¡®ç‡Ã—å¬å›ç‡/(ç²¾ç¡®ç‡+å¬å›ç‡) | ç»¼åˆè€ƒè™‘ |
| **AUC-ROC** | ROCæ›²çº¿ä¸‹é¢ç§¯ | è¯„ä¼°æ•´ä½“æ€§èƒ½ |

```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# è®¡ç®—å„é¡¹æŒ‡æ ‡
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print(f"å‡†ç¡®ç‡: {accuracy:.4f}")
print(f"ç²¾ç¡®ç‡: {precision:.4f}")
print(f"å¬å›ç‡: {recall:.4f}")
print(f"F1åˆ†æ•°: {f1:.4f}")

# ROCæ›²çº¿
from sklearn.metrics import roc_curve, auc
y_pred_proba = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('å‡æ­£ç‡ (False Positive Rate)')
plt.ylabel('çœŸæ­£ç‡ (True Positive Rate)')
plt.title('ROCæ›²çº¿')
plt.legend(loc="lower right")
plt.show()
```

### å›å½’æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

```python
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# é¢„æµ‹
y_pred = model.predict(X_test)

# è®¡ç®—æŒ‡æ ‡
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE): {rmse:.4f}")
print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.4f}")
print(f"RÂ² åˆ†æ•°: {r2:.4f}")

# å¯è§†åŒ–é¢„æµ‹ç»“æœ
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('çœŸå®å€¼')
plt.ylabel('é¢„æµ‹å€¼')
plt.title('çœŸå®å€¼ vs é¢„æµ‹å€¼')
plt.show()
```

---

## 9ï¸âƒ£ æ¨¡å‹éƒ¨ç½²ï¼ˆModel Deploymentï¼‰

### ä¿å­˜æ¨¡å‹

```python
import joblib
import pickle

# æ–¹æ³•1ï¼šä½¿ç”¨joblibï¼ˆæ¨èï¼‰
joblib.dump(model, 'model.joblib')

# æ–¹æ³•2ï¼šä½¿ç”¨pickle
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

# åŠ è½½æ¨¡å‹
loaded_model = joblib.load('model.joblib')

# ä½¿ç”¨åŠ è½½çš„æ¨¡å‹
predictions = loaded_model.predict(new_data)
```

### éƒ¨ç½²æ–¹å¼

#### 1. æ‰¹é‡é¢„æµ‹

```python
# å®šæœŸæ‰¹é‡å¤„ç†
def batch_predict(data_file, model_file, output_file):
    # åŠ è½½æ¨¡å‹
    model = joblib.load(model_file)
    
    # åŠ è½½æ–°æ•°æ®
    new_data = pd.read_csv(data_file)
    
    # é¢„å¤„ç†
    X_new = preprocessor.transform(new_data)
    
    # é¢„æµ‹
    predictions = model.predict(X_new)
    
    # ä¿å­˜ç»“æœ
    new_data['prediction'] = predictions
    new_data.to_csv(output_file, index=False)
    
    return predictions
```

#### 2. å®æ—¶é¢„æµ‹ï¼ˆAPIï¼‰

```python
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

# åŠ è½½æ¨¡å‹
model = joblib.load('model.joblib')
preprocessor = joblib.load('preprocessor.joblib')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # è·å–è¾“å…¥æ•°æ®
        data = request.get_json()
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame([data])
        
        # é¢„å¤„ç†
        X = preprocessor.transform(df)
        
        # é¢„æµ‹
        prediction = model.predict(X)[0]
        probability = model.predict_proba(X)[0].tolist()
        
        # è¿”å›ç»“æœ
        return jsonify({
            'prediction': int(prediction),
            'probability': probability,
            'success': True
        })
    
    except Exception as e:
        return jsonify({
            'error': str(e),
            'success': False
        }), 400

if __name__ == '__main__':
    app.run(debug=True, port=5000)
```

**è°ƒç”¨API**ï¼š

```python
import requests

# å‡†å¤‡æ•°æ®
data = {
    'feature1': 25,
    'feature2': 'A',
    'feature3': 100.5
}

# å‘é€è¯·æ±‚
response = requests.post('http://localhost:5000/predict', json=data)

# è·å–ç»“æœ
result = response.json()
print(f"é¢„æµ‹ç»“æœ: {result['prediction']}")
print(f"æ¦‚ç‡: {result['probability']}")
```

---

## ğŸ”Ÿ ç›‘æ§ç»´æŠ¤ï¼ˆMonitoring & Maintenanceï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦ç›‘æ§ï¼Ÿ

æ¨¡å‹ä¸Šçº¿åå¯èƒ½é¢ä¸´çš„é—®é¢˜ï¼š
- ğŸ“‰ **æ€§èƒ½ä¸‹é™**ï¼šæ•°æ®åˆ†å¸ƒå˜åŒ–
- ğŸ› **æ•°æ®è´¨é‡**ï¼šå¼‚å¸¸è¾“å…¥
- âš¡ **å“åº”å»¶è¿Ÿ**ï¼šç³»ç»Ÿè´Ÿè½½
- ğŸ”’ **å®‰å…¨é—®é¢˜**ï¼šæ¶æ„è¾“å…¥

### ç›‘æ§å†…å®¹

#### 1. æ€§èƒ½ç›‘æ§

```python
import time
from datetime import datetime

class ModelMonitor:
    def __init__(self):
        self.predictions = []
        self.latencies = []
        self.errors = []
    
    def log_prediction(self, input_data, prediction, actual=None):
        start_time = time.time()
        
        # è®°å½•é¢„æµ‹
        log_entry = {
            'timestamp': datetime.now(),
            'prediction': prediction,
            'actual': actual,
            'latency': time.time() - start_time
        }
        
        self.predictions.append(log_entry)
        
        # å¦‚æœæœ‰å®é™…å€¼ï¼Œè®¡ç®—å‡†ç¡®ç‡
        if actual is not None:
            is_correct = (prediction == actual)
            return is_correct
    
    def get_statistics(self):
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if len(self.predictions) == 0:
            return None
        
        correct = sum(1 for p in self.predictions if p.get('actual') and p['prediction'] == p['actual'])
        total = sum(1 for p in self.predictions if p.get('actual'))
        
        return {
            'total_predictions': len(self.predictions),
            'accuracy': correct / total if total > 0 else None,
            'avg_latency': np.mean(self.latencies) if self.latencies else None
        }

# ä½¿ç”¨ç›‘æ§
monitor = ModelMonitor()

# è®°å½•é¢„æµ‹
monitor.log_prediction(input_data, prediction, actual_label)

# æŸ¥çœ‹ç»Ÿè®¡
stats = monitor.get_statistics()
print(stats)
```

#### 2. æ•°æ®æ¼‚ç§»æ£€æµ‹

```python
from scipy.stats import ks_2samp

def detect_data_drift(reference_data, current_data, threshold=0.05):
    """
    ä½¿ç”¨K-Sæ£€éªŒæ£€æµ‹æ•°æ®æ¼‚ç§»
    """
    results = {}
    
    for column in reference_data.columns:
        # K-Sæ£€éªŒ
        statistic, p_value = ks_2samp(
            reference_data[column],
            current_data[column]
        )
        
        # åˆ¤æ–­æ˜¯å¦æ¼‚ç§»
        is_drift = p_value < threshold
        
        results[column] = {
            'statistic': statistic,
            'p_value': p_value,
            'is_drift': is_drift
        }
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
drift_results = detect_data_drift(training_data, production_data)

# æŠ¥è­¦
for feature, result in drift_results.items():
    if result['is_drift']:
        print(f"âš ï¸ è­¦å‘Š: ç‰¹å¾ '{feature}' å‘ç”Ÿæ•°æ®æ¼‚ç§»ï¼")
```

### æ¨¡å‹æ›´æ–°ç­–ç•¥

1. **å®šæœŸé‡è®­ç»ƒ**ï¼šæ¯å‘¨/æœˆç”¨æ–°æ•°æ®é‡æ–°è®­ç»ƒ
2. **åœ¨çº¿å­¦ä¹ **ï¼šæŒç»­ä»æ–°æ•°æ®ä¸­å­¦ä¹ 
3. **A/Bæµ‹è¯•**ï¼šåŒæ—¶è¿è¡Œå¤šä¸ªæ¨¡å‹ç‰ˆæœ¬
4. **æ¸è¿›å¼æ›´æ–°**ï¼šé€æ­¥æ›¿æ¢æ—§æ¨¡å‹

---

## ğŸ“Š å®Œæ•´é¡¹ç›®ç¤ºä¾‹

### æˆ¿ä»·é¢„æµ‹é¡¹ç›®

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import joblib

# ========== 1. é—®é¢˜å®šä¹‰ ==========
# ç›®æ ‡ï¼šæ ¹æ®æˆ¿å±‹ç‰¹å¾é¢„æµ‹æˆ¿ä»·
# ç±»å‹ï¼šå›å½’é—®é¢˜
# æŒ‡æ ‡ï¼šRMSEã€RÂ²

# ========== 2. æ•°æ®æ”¶é›† ==========
data = pd.read_csv('housing_data.csv')
print(f"æ•°æ®å½¢çŠ¶: {data.shape}")

# ========== 3. æ•°æ®æ¢ç´¢ ==========
print("\næ•°æ®æ¦‚è§ˆ:")
print(data.head())
print("\nç»Ÿè®¡ä¿¡æ¯:")
print(data.describe())
print("\nç¼ºå¤±å€¼:")
print(data.isnull().sum())

# ========== 4. æ•°æ®é¢„å¤„ç† ==========
# å¤„ç†ç¼ºå¤±å€¼
data = data.dropna()

# ç‰¹å¾å’Œç›®æ ‡åˆ†ç¦»
X = data.drop('price', axis=1)
y = data['price']

# åˆ’åˆ†æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ç‰¹å¾ç¼©æ”¾
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ========== 5. ç‰¹å¾å·¥ç¨‹ ==========
# ï¼ˆæ ¹æ®éœ€è¦æ·»åŠ ï¼‰

# ========== 6. æ¨¡å‹é€‰æ‹© ==========
model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42
)

# ========== 7. æ¨¡å‹è®­ç»ƒ ==========
print("\nå¼€å§‹è®­ç»ƒ...")
model.fit(X_train_scaled, y_train)
print("è®­ç»ƒå®Œæˆï¼")

# ========== 8. æ¨¡å‹è¯„ä¼° ==========
y_train_pred = model.predict(X_train_scaled)
y_test_pred = model.predict(X_test_scaled)

train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)

print(f"\nè®­ç»ƒé›† RMSE: {train_rmse:.2f}")
print(f"æµ‹è¯•é›† RMSE: {test_rmse:.2f}")
print(f"è®­ç»ƒé›† RÂ²: {train_r2:.4f}")
print(f"æµ‹è¯•é›† RÂ²: {test_r2:.4f}")

# ========== 9. æ¨¡å‹éƒ¨ç½² ==========
# ä¿å­˜æ¨¡å‹
joblib.dump(model, 'housing_model.joblib')
joblib.dump(scaler, 'scaler.joblib')
print("\næ¨¡å‹å·²ä¿å­˜ï¼")

# ========== 10. ä½¿ç”¨æ¨¡å‹ ==========
def predict_price(features):
    """é¢„æµ‹æˆ¿ä»·"""
    model = joblib.load('housing_model.joblib')
    scaler = joblib.load('scaler.joblib')
    
    features_scaled = scaler.transform([features])
    prediction = model.predict(features_scaled)[0]
    
    return prediction

# æµ‹è¯•
new_house = [2000, 3, 2, 2010, 5000]  # é¢ç§¯ã€å§å®¤ã€æµ´å®¤ã€å¹´ä»½ã€åœ°å—
predicted_price = predict_price(new_house)
print(f"\né¢„æµ‹æˆ¿ä»·: ${predicted_price:,.2f}")
```

---

## ğŸ“ è¯¾åç»ƒä¹ 

### é€‰æ‹©é¢˜

#### 1. æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­æœ€è€—æ—¶çš„æ­¥éª¤é€šå¸¸æ˜¯ï¼Ÿ

```quiz-json
{
  "question": "æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­æœ€è€—æ—¶çš„æ­¥éª¤é€šå¸¸æ˜¯ï¼Ÿ",
  "type": "single",
  "options": [
    {
      "id": "a",
      "text": "æ¨¡å‹é€‰æ‹©",
      "isCorrect": false
    },
    {
      "id": "b",
      "text": "æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹",
      "isCorrect": true
    },
    {
      "id": "c",
      "text": "æ¨¡å‹è®­ç»ƒ",
      "isCorrect": false
    },
    {
      "id": "d",
      "text": "æ¨¡å‹éƒ¨ç½²",
      "isCorrect": false
    }
  ],
  "explanation": "æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾å·¥ç¨‹é€šå¸¸å æ®æ•´ä¸ªé¡¹ç›®60-80%çš„æ—¶é—´ã€‚é«˜è´¨é‡çš„æ•°æ®å’Œç‰¹å¾æ˜¯æ¨¡å‹æˆåŠŸçš„å…³é”®ï¼Œéœ€è¦å¤§é‡æ—¶é—´è¿›è¡Œæ¸…æ´—ã€è½¬æ¢å’Œåˆ›å»ºæ–°ç‰¹å¾ã€‚"
}
```

#### 2. å¤„ç†ç¼ºå¤±å€¼æ—¶ï¼Œå“ªç§æƒ…å†µé€‚åˆä½¿ç”¨ä¸­ä½æ•°å¡«å……ï¼Ÿ

```quiz-json
{
  "question": "å¤„ç†ç¼ºå¤±å€¼æ—¶ï¼Œå“ªç§æƒ…å†µé€‚åˆä½¿ç”¨ä¸­ä½æ•°å¡«å……ï¼Ÿ",
  "type": "single",
  "options": [
    {
      "id": "a",
      "text": "ç±»åˆ«å‹ç‰¹å¾",
      "isCorrect": false
    },
    {
      "id": "b",
      "text": "æ•°å€¼å‹ç‰¹å¾ï¼Œä¸”æ•°æ®å‘ˆæ­£æ€åˆ†å¸ƒ",
      "isCorrect": false
    },
    {
      "id": "c",
      "text": "æ•°å€¼å‹ç‰¹å¾ï¼Œä¸”æ•°æ®ä¸­æœ‰å¼‚å¸¸å€¼",
      "isCorrect": true
    },
    {
      "id": "d",
      "text": "æ—¶é—´åºåˆ—æ•°æ®",
      "isCorrect": false
    }
  ],
  "explanation": "å½“æ•°æ®ä¸­å­˜åœ¨å¼‚å¸¸å€¼æ—¶ï¼Œä¸­ä½æ•°æ¯”å‡å€¼æ›´ç¨³å¥ã€‚å¼‚å¸¸å€¼ä¼šæ˜¾è‘—å½±å“å‡å€¼ï¼Œä½†å¯¹ä¸­ä½æ•°çš„å½±å“è¾ƒå°ï¼Œå› æ­¤ä¸­ä½æ•°å¡«å……æ›´é€‚åˆæœ‰å¼‚å¸¸å€¼çš„æƒ…å†µã€‚"
}
```

#### 3. ç‰¹å¾ç¼©æ”¾å¯¹å“ªäº›ç®—æ³•æœ€é‡è¦ï¼Ÿ

```quiz-json
{
  "question": "ç‰¹å¾ç¼©æ”¾å¯¹å“ªäº›ç®—æ³•æœ€é‡è¦ï¼Ÿ",
  "type": "multiple",
  "options": [
    {
      "id": "a",
      "text": "çº¿æ€§å›å½’",
      "isCorrect": true
    },
    {
      "id": "b",
      "text": "å†³ç­–æ ‘",
      "isCorrect": false
    },
    {
      "id": "c",
      "text": "Kè¿‘é‚»(KNN)",
      "isCorrect": true
    },
    {
      "id": "d",
      "text": "æ”¯æŒå‘é‡æœº(SVM)",
      "isCorrect": true
    },
    {
      "id": "e",
      "text": "éšæœºæ£®æ—",
      "isCorrect": false
    }
  ],
  "explanation": "åŸºäºè·ç¦»çš„ç®—æ³•(KNNã€SVM)å’Œæ¢¯åº¦ä¸‹é™ä¼˜åŒ–çš„ç®—æ³•(çº¿æ€§å›å½’ã€ç¥ç»ç½‘ç»œ)å¯¹ç‰¹å¾å°ºåº¦æ•æ„Ÿã€‚è€ŒåŸºäºæ ‘çš„ç®—æ³•(å†³ç­–æ ‘ã€éšæœºæ£®æ—)ä¸å—ç‰¹å¾å°ºåº¦å½±å“ï¼Œå› ä¸ºå®ƒä»¬åŸºäºç‰¹å¾å€¼çš„ç›¸å¯¹é¡ºåºè¿›è¡Œåˆ†è£‚ã€‚"
}
```

#### 4. ä¸‹åˆ—å“ªäº›æ˜¯è¯„ä¼°åˆ†ç±»æ¨¡å‹çš„æŒ‡æ ‡ï¼Ÿ

```quiz-json
{
  "question": "ä¸‹åˆ—å“ªäº›æ˜¯è¯„ä¼°åˆ†ç±»æ¨¡å‹çš„æŒ‡æ ‡ï¼Ÿ",
  "type": "multiple",
  "options": [
    {
      "id": "a",
      "text": "å‡†ç¡®ç‡(Accuracy)",
      "isCorrect": true
    },
    {
      "id": "b",
      "text": "F1åˆ†æ•°",
      "isCorrect": true
    },
    {
      "id": "c",
      "text": "å‡æ–¹è¯¯å·®(MSE)",
      "isCorrect": false
    },
    {
      "id": "d",
      "text": "AUC-ROC",
      "isCorrect": true
    },
    {
      "id": "e",
      "text": "RÂ²åˆ†æ•°",
      "isCorrect": false
    }
  ],
  "explanation": "å‡†ç¡®ç‡ã€F1åˆ†æ•°å’ŒAUC-ROCæ˜¯åˆ†ç±»é—®é¢˜çš„è¯„ä¼°æŒ‡æ ‡ã€‚MSEå’ŒRÂ²åˆ†æ•°æ˜¯å›å½’é—®é¢˜çš„è¯„ä¼°æŒ‡æ ‡ï¼Œä¸é€‚ç”¨äºåˆ†ç±»é—®é¢˜ã€‚"
}
```

#### 5. æ•°æ®æ¼‚ç§»(Data Drift)æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Ÿ

```quiz-json
{
  "question": "æ•°æ®æ¼‚ç§»(Data Drift)æŒ‡çš„æ˜¯ä»€ä¹ˆï¼Ÿ",
  "type": "single",
  "options": [
    {
      "id": "a",
      "text": "è®­ç»ƒæ•°æ®ä¸­çš„ç¼ºå¤±å€¼",
      "isCorrect": false
    },
    {
      "id": "b",
      "text": "ç”Ÿäº§ç¯å¢ƒä¸­çš„æ•°æ®åˆ†å¸ƒä¸è®­ç»ƒæ•°æ®ä¸åŒ",
      "isCorrect": true
    },
    {
      "id": "c",
      "text": "æ¨¡å‹é¢„æµ‹é”™è¯¯",
      "isCorrect": false
    },
    {
      "id": "d",
      "text": "æ•°æ®å­˜å‚¨é”™è¯¯",
      "isCorrect": false
    }
  ],
  "explanation": "æ•°æ®æ¼‚ç§»æ˜¯æŒ‡ç”Ÿäº§ç¯å¢ƒä¸­çš„æ•°æ®åˆ†å¸ƒéšæ—¶é—´å‘ç”Ÿå˜åŒ–ï¼Œä¸è®­ç»ƒæ—¶çš„æ•°æ®åˆ†å¸ƒä¸åŒã€‚è¿™ä¼šå¯¼è‡´æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œéœ€è¦ç›‘æ§å¹¶å®šæœŸé‡æ–°è®­ç»ƒæ¨¡å‹ã€‚"
}
```

### å®è·µé¢˜

#### 6. å®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹

ç¼–å†™ä»£ç å®Œæˆä»¥ä¸‹æ•°æ®é¢„å¤„ç†ä»»åŠ¡ï¼š
1. åŠ è½½æ•°æ®
2. æ£€æŸ¥ç¼ºå¤±å€¼å¹¶å¤„ç†
3. å¤„ç†å¼‚å¸¸å€¼
4. ç‰¹å¾ç¼–ç 
5. ç‰¹å¾ç¼©æ”¾
6. åˆ’åˆ†æ•°æ®é›†

#### 7. æ¨¡å‹æ¯”è¾ƒ

ä½¿ç”¨åŒä¸€æ•°æ®é›†è®­ç»ƒä»¥ä¸‹æ¨¡å‹å¹¶æ¯”è¾ƒæ€§èƒ½ï¼š
- é€»è¾‘å›å½’
- å†³ç­–æ ‘
- éšæœºæ£®æ—
- æ”¯æŒå‘é‡æœº

ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾è¡¨ã€‚

#### 8. ç‰¹å¾é‡è¦æ€§åˆ†æ

è®­ç»ƒä¸€ä¸ªéšæœºæ£®æ—æ¨¡å‹ï¼Œåˆ†æå¹¶å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§ï¼Œæ‰¾å‡ºå¯¹é¢„æµ‹æœ€é‡è¦çš„å‰10ä¸ªç‰¹å¾ã€‚

---

## ğŸ¯ å­¦ä¹ è¦ç‚¹æ€»ç»“

### æ ¸å¿ƒæµç¨‹è®°å¿†

```
ğŸ¯ é—®é¢˜å®šä¹‰ â†’ æ˜ç¡®ç›®æ ‡å’ŒæˆåŠŸæ ‡å‡†
ğŸ“Š æ•°æ®æ”¶é›† â†’ è·å–è¶³å¤Ÿè´¨é‡çš„æ•°æ®
ğŸ” æ•°æ®æ¢ç´¢ â†’ äº†è§£æ•°æ®åˆ†å¸ƒå’Œç‰¹å¾
ğŸ§¹ æ•°æ®é¢„å¤„ç† â†’ æ¸…æ´—å’Œè½¬æ¢æ•°æ®
âš™ï¸ ç‰¹å¾å·¥ç¨‹ â†’ åˆ›é€ å’Œé€‰æ‹©æœ€ä½³ç‰¹å¾
ğŸ¤– æ¨¡å‹é€‰æ‹© â†’ æ ¹æ®é—®é¢˜é€‰æ‹©ç®—æ³•
ğŸ“š æ¨¡å‹è®­ç»ƒ â†’ è®­ç»ƒå¹¶è°ƒä¼˜æ¨¡å‹
ğŸ“ˆ æ¨¡å‹è¯„ä¼° â†’ ç”¨åˆé€‚æŒ‡æ ‡è¯„ä¼°æ€§èƒ½
ğŸš€ æ¨¡å‹éƒ¨ç½² â†’ å°†æ¨¡å‹æŠ•å…¥ç”Ÿäº§
ğŸ‘€ ç›‘æ§ç»´æŠ¤ â†’ æŒç»­ç›‘æ§å’Œæ›´æ–°
```

### å…³é”®è¦ç‚¹

1. **æ•°æ®è´¨é‡å†³å®šæ¨¡å‹ä¸Šé™**
   - Garbage In, Garbage Out
   - æ•°æ®é¢„å¤„ç†å 60-80%æ—¶é—´

2. **ä»ç®€å•å¼€å§‹**
   - å…ˆå»ºç«‹åŸºçº¿æ¨¡å‹
   - é€æ­¥å¢åŠ å¤æ‚åº¦

3. **ç‰¹å¾å·¥ç¨‹å¾ˆé‡è¦**
   - å¥½çš„ç‰¹å¾ > å¤æ‚çš„ç®—æ³•
   - é¢†åŸŸçŸ¥è¯†è‡³å…³é‡è¦

4. **é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡**
   - ä¸åŒé—®é¢˜ç”¨ä¸åŒæŒ‡æ ‡
   - è€ƒè™‘ä¸šåŠ¡ç›®æ ‡

5. **æ¨¡å‹éœ€è¦æŒç»­ç»´æŠ¤**
   - ç›‘æ§æ€§èƒ½å˜åŒ–
   - å®šæœŸæ›´æ–°æ¨¡å‹

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

### æ¨èèµ„æº

**ä¹¦ç±**ï¼š
- ã€Šæœºå™¨å­¦ä¹ å®æˆ˜ã€‹
- ã€ŠPythonæœºå™¨å­¦ä¹ ã€‹
- ã€Šç‰¹å¾å·¥ç¨‹å…¥é—¨ä¸å®è·µã€‹

**åœ¨çº¿èµ„æº**ï¼š
- Scikit-learnå®˜æ–¹æ–‡æ¡£
- Kaggleç«èµ›å’Œæ•™ç¨‹
- Google ML Crash Course

**å®è·µé¡¹ç›®**ï¼š
- Kaggleå…¥é—¨ç«èµ›
- UCIæœºå™¨å­¦ä¹ æ•°æ®é›†
- è‡ªå·±çš„å®é™…é—®é¢˜

### ä¸‹ä¸€æ­¥å­¦ä¹ 

**æ¨èå­¦ä¹ è·¯å¾„**ï¼š

1. âœ… **å·²å®Œæˆ**ï¼šäº†è§£å®Œæ•´æµç¨‹
2. ğŸ“– **ä¸‹ä¸€æ­¥**ï¼šæ·±å…¥å­¦ä¹ æ•°æ®é›†åˆ’åˆ†
3. ğŸ¯ **åç»­**ï¼šå­¦ä¹ å„ç±»ç®—æ³•ç»†èŠ‚
4. ğŸ’ª **è¿›é˜¶**ï¼šå‚ä¸å®é™…é¡¹ç›®

---

## âœ… ä¸‹èŠ‚é¢„å‘Š

**ç¬¬4è¯¾ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†**

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†è¯¦ç»†å­¦ä¹ ï¼š
- ä¸ºä»€ä¹ˆè¦åˆ’åˆ†æ•°æ®é›†
- å¦‚ä½•æ­£ç¡®åˆ’åˆ†æ•°æ®é›†
- äº¤å‰éªŒè¯çš„åŸç†å’Œåº”ç”¨
- é¿å…æ•°æ®æ³„éœ²çš„æŠ€å·§

**é¢„ä¹ ä»»åŠ¡**ï¼š
- æ€è€ƒï¼šä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥ç”¨è®­ç»ƒæ•°æ®è¯„ä¼°æ¨¡å‹ï¼Ÿ
- äº†è§£ï¼šä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆï¼Ÿ
- å‡†å¤‡ï¼šä¸€ä¸ªæƒ³è¦è§£å†³çš„å®é™…é—®é¢˜

---

**æ­å–œä½ å®Œæˆç¬¬3è¯¾ï¼** ğŸ‰

ä½ ç°åœ¨å·²ç»æŒæ¡äº†å®Œæ•´çš„æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ã€‚è®°ä½ï¼Œå®è·µæ˜¯æœ€å¥½çš„å­¦ä¹ æ–¹å¼ï¼Œå¿«å»åŠ¨æ‰‹è¯•è¯•å§ï¼ğŸ’ª

