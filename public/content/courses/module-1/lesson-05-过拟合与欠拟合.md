# ç¬¬5è¯¾ï¼šè¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ

## ğŸ“– è¯¾ç¨‹ç›®æ ‡

å­¦å®Œæœ¬è¯¾ï¼Œä½ å°†èƒ½å¤Ÿï¼š
- âœ… ç†è§£è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„æ¦‚å¿µ
- âœ… è¯†åˆ«æ¨¡å‹æ˜¯å¦è¿‡æ‹Ÿåˆæˆ–æ¬ æ‹Ÿåˆ
- âœ… æŒæ¡é˜²æ­¢è¿‡æ‹Ÿåˆçš„å¸¸ç”¨æ–¹æ³•
- âœ… äº†è§£åå·®-æ–¹å·®æƒè¡¡åŸç†

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š50-55åˆ†é’Ÿ  
**éš¾åº¦ç­‰çº§**ï¼šâ­â­â­ è¿›é˜¶

---

## ğŸ¯ ä»€ä¹ˆæ˜¯è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆï¼Ÿ

### ä¸€ä¸ªç”ŸåŠ¨çš„æ¯”å–»

æƒ³è±¡ä½ åœ¨å‡†å¤‡è€ƒè¯•ï¼š

#### ğŸ“š æ­»è®°ç¡¬èƒŒçš„å­¦ç”Ÿï¼ˆè¿‡æ‹Ÿåˆï¼‰

**å­¦ä¹ æ–¹å¼**ï¼š
- æŠŠæ‰€æœ‰ç»ƒä¹ é¢˜çš„ç­”æ¡ˆéƒ½èƒŒä¸‹æ¥
- å®Œå…¨è®°ä½æ¯é“é¢˜çš„è§£æ³•
- ç»ƒä¹ é¢˜æµ‹è¯•ï¼š100åˆ† âœ…

**è€ƒè¯•ç»“æœ**ï¼š
- é‡åˆ°æ–°é¢˜å‹ï¼šä¸ä¼šåš âŒ
- é¢˜ç›®ç¨å¾®å˜åŒ–ï¼šå®Œå…¨æ‡µäº† âŒ
- æœ€ç»ˆæˆç»©ï¼šä¸åŠæ ¼ ğŸ˜±

**é—®é¢˜**ï¼šåªä¼šåšè§è¿‡çš„é¢˜ï¼Œä¸ç†è§£çŸ¥è¯†åŸç†ï¼

---

#### ğŸ˜´ æ•·è¡å­¦ä¹ çš„å­¦ç”Ÿï¼ˆæ¬ æ‹Ÿåˆï¼‰

**å­¦ä¹ æ–¹å¼**ï¼š
- åªçœ‹äº†è¯¾æœ¬å‰å‡ é¡µ
- åŸºæœ¬æ¦‚å¿µéƒ½æ²¡ææ‡‚
- ç»ƒä¹ é¢˜æµ‹è¯•ï¼š40åˆ† âŒ

**è€ƒè¯•ç»“æœ**ï¼š
- åŸºç¡€é¢˜ï¼šä¸å¤ªä¼šåš âŒ
- ç¨éš¾çš„é¢˜ï¼šå®Œå…¨ä¸ä¼š âŒ
- æœ€ç»ˆæˆç»©ï¼šä¸åŠæ ¼ ğŸ˜¢

**é—®é¢˜**ï¼šå­¦ä¹ ä¸å¤Ÿï¼ŒåŸºæœ¬çŸ¥è¯†éƒ½æ²¡æŒæ¡ï¼

---

#### ğŸ“ ç†è§£åŸç†çš„å­¦ç”Ÿï¼ˆåˆšåˆšå¥½ï¼‰

**å­¦ä¹ æ–¹å¼**ï¼š
- ç†è§£åŸºæœ¬æ¦‚å¿µå’ŒåŸç†
- é€šè¿‡ç»ƒä¹ é¢˜å·©å›ºçŸ¥è¯†
- ç»ƒä¹ é¢˜æµ‹è¯•ï¼š85åˆ† âœ…

**è€ƒè¯•ç»“æœ**ï¼š
- åŸºç¡€é¢˜ï¼šåšå¾—å¾ˆå¥½ âœ…
- æ–°é¢˜å‹ï¼šèƒ½ä¸¾ä¸€åä¸‰ âœ…
- æœ€ç»ˆæˆç»©ï¼šä¼˜ç§€ ğŸ‰

**å…³é”®**ï¼šæ—¢æŒæ¡çŸ¥è¯†ï¼Œåˆèƒ½çµæ´»åº”ç”¨ï¼

---

## ğŸ” æœºå™¨å­¦ä¹ ä¸­çš„è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ

### æ ¸å¿ƒæ¦‚å¿µ

![è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆæ¦‚å¿µå›¾](/content/images/lesson-05-è¿‡æ‹Ÿåˆæ¬ æ‹Ÿåˆæ¦‚å¿µ.png)

### å®šä¹‰

**æ¬ æ‹Ÿåˆï¼ˆUnderfittingï¼‰**ï¼š
- æ¨¡å‹å¤ªç®€å•ï¼Œè¿è®­ç»ƒæ•°æ®éƒ½å­¦ä¸å¥½
- è®­ç»ƒé›†è¡¨ç°å·®ï¼Œæµ‹è¯•é›†è¡¨ç°ä¹Ÿå·®
- æ¨¡å‹æ²¡æœ‰å……åˆ†å­¦ä¹ æ•°æ®çš„æ¨¡å¼

**è¿‡æ‹Ÿåˆï¼ˆOverfittingï¼‰**ï¼š
- æ¨¡å‹å¤ªå¤æ‚ï¼Œè®°ä½äº†è®­ç»ƒæ•°æ®çš„ç»†èŠ‚ï¼ˆåŒ…æ‹¬å™ªå£°ï¼‰
- è®­ç»ƒé›†è¡¨ç°å¾ˆå¥½ï¼Œæµ‹è¯•é›†è¡¨ç°å·®
- æ¨¡å‹å¤±å»äº†æ³›åŒ–èƒ½åŠ›

**åˆšåˆšå¥½ï¼ˆGood Fitï¼‰**ï¼š
- æ¨¡å‹å¤æ‚åº¦é€‚ä¸­
- è®­ç»ƒé›†è¡¨ç°å¥½ï¼Œæµ‹è¯•é›†è¡¨ç°ä¹Ÿå¥½
- æ¨¡å‹å…·æœ‰è‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›

---

## ğŸ“Š å¯è§†åŒ–ç†è§£

### ä¾‹å­ï¼šæ‹Ÿåˆæ›²çº¿

å‡è®¾æˆ‘ä»¬è¦æ‹Ÿåˆä»¥ä¸‹æ•°æ®ç‚¹ï¼š

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
np.random.seed(42)
X = np.linspace(0, 10, 20).reshape(-1, 1)
y = 3*X.ravel() + 5 + np.random.randn(20) * 3

# ç»˜åˆ¶æ•°æ®ç‚¹
plt.figure(figsize=(15, 4))

# 1. æ¬ æ‹Ÿåˆ - ä¸€æ¬¡å¤šé¡¹å¼ï¼ˆç›´çº¿ï¼‰
plt.subplot(131)
model_underfit = LinearRegression()
model_underfit.fit(X, y)
X_test = np.linspace(0, 10, 100).reshape(-1, 1)
y_pred = model_underfit.predict(X_test)

plt.scatter(X, y, color='blue', alpha=0.6, label='è®­ç»ƒæ•°æ®')
plt.plot(X_test, y_pred, 'r-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
plt.title('æ¬ æ‹Ÿåˆï¼ˆå¤ªç®€å•ï¼‰', fontsize=14, fontweight='bold')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# 2. åˆšåˆšå¥½ - ä¸‰æ¬¡å¤šé¡¹å¼
plt.subplot(132)
model_good = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('linear', LinearRegression())
])
model_good.fit(X, y)
y_pred = model_good.predict(X_test)

plt.scatter(X, y, color='blue', alpha=0.6, label='è®­ç»ƒæ•°æ®')
plt.plot(X_test, y_pred, 'g-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
plt.title('åˆšåˆšå¥½ï¼ˆåˆé€‚ï¼‰', fontsize=14, fontweight='bold')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)

# 3. è¿‡æ‹Ÿåˆ - åäº”æ¬¡å¤šé¡¹å¼
plt.subplot(133)
model_overfit = Pipeline([
    ('poly', PolynomialFeatures(degree=15)),
    ('linear', LinearRegression())
])
model_overfit.fit(X, y)
y_pred = model_overfit.predict(X_test)

plt.scatter(X, y, color='blue', alpha=0.6, label='è®­ç»ƒæ•°æ®')
plt.plot(X_test, y_pred, 'orange', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
plt.title('è¿‡æ‹Ÿåˆï¼ˆå¤ªå¤æ‚ï¼‰', fontsize=14, fontweight='bold')
plt.xlabel('X')
plt.ylabel('y')
plt.legend()
plt.grid(True, alpha=0.3)
plt.ylim(-10, 50)

plt.tight_layout()
plt.show()

# è®¡ç®—è®­ç»ƒè¯¯å·®å’Œæµ‹è¯•è¯¯å·®
from sklearn.metrics import mean_squared_error

print("æ¬ æ‹Ÿåˆæ¨¡å‹:")
print(f"è®­ç»ƒMSE: {mean_squared_error(y, model_underfit.predict(X)):.2f}")

print("\nåˆšåˆšå¥½æ¨¡å‹:")
print(f"è®­ç»ƒMSE: {mean_squared_error(y, model_good.predict(X)):.2f}")

print("\nè¿‡æ‹Ÿåˆæ¨¡å‹:")
print(f"è®­ç»ƒMSE: {mean_squared_error(y, model_overfit.predict(X)):.2f}")
```

**è¾“å‡ºåˆ†æ**ï¼š

| æ¨¡å‹ | è®­ç»ƒè¯¯å·® | æµ‹è¯•è¯¯å·® | è¯Šæ–­ |
|------|---------|---------|------|
| æ¬ æ‹Ÿåˆ | é«˜ | é«˜ | æ¨¡å‹å¤ªç®€å• |
| åˆšåˆšå¥½ | ä¸­ | ä¸­ | å¹³è¡¡è‰¯å¥½ |
| è¿‡æ‹Ÿåˆ | å¾ˆä½ | å¾ˆé«˜ | æ¨¡å‹å¤ªå¤æ‚ |

---

## ğŸ” å¦‚ä½•è¯Šæ–­è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆï¼Ÿ

### æ–¹æ³•ä¸€ï¼šè§‚å¯Ÿå­¦ä¹ æ›²çº¿

å­¦ä¹ æ›²çº¿å±•ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒè®­ç»ƒæ ·æœ¬æ•°é‡ä¸‹çš„æ€§èƒ½ï¼š

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

def plot_learning_curves(estimator, X, y, title):
    """
    ç»˜åˆ¶å­¦ä¹ æ›²çº¿
    """
    train_sizes, train_scores, val_scores = learning_curve(
        estimator, X, y, 
        train_sizes=np.linspace(0.1, 1.0, 10),
        cv=5, 
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    train_mean = -train_scores.mean(axis=1)
    train_std = train_scores.std(axis=1)
    val_mean = -val_scores.mean(axis=1)
    val_std = val_scores.std(axis=1)
    
    # ç»˜å›¾
    plt.figure(figsize=(10, 6))
    plt.plot(train_sizes, train_mean, 'o-', color='r', label='è®­ç»ƒé›†è¯¯å·®')
    plt.plot(train_sizes, val_mean, 'o-', color='g', label='éªŒè¯é›†è¯¯å·®')
    
    plt.fill_between(train_sizes, train_mean - train_std, 
                     train_mean + train_std, alpha=0.1, color='r')
    plt.fill_between(train_sizes, val_mean - val_std, 
                     val_mean + val_std, alpha=0.1, color='g')
    
    plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°é‡', fontsize=12)
    plt.ylabel('å‡æ–¹è¯¯å·®', fontsize=12)
    plt.title(title, fontsize=14, fontweight='bold')
    plt.legend(loc='best', fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.show()

# ä½¿ç”¨ç¤ºä¾‹
from sklearn.tree import DecisionTreeRegressor

# æ¬ æ‹Ÿåˆæ¨¡å‹ï¼ˆmax_depth=1ï¼‰
plot_learning_curves(
    DecisionTreeRegressor(max_depth=1, random_state=42),
    X, y, 
    'æ¬ æ‹Ÿåˆæ¨¡å‹çš„å­¦ä¹ æ›²çº¿'
)

# åˆšåˆšå¥½æ¨¡å‹ï¼ˆmax_depth=3ï¼‰
plot_learning_curves(
    DecisionTreeRegressor(max_depth=3, random_state=42),
    X, y, 
    'è‰¯å¥½æ‹Ÿåˆæ¨¡å‹çš„å­¦ä¹ æ›²çº¿'
)

# è¿‡æ‹Ÿåˆæ¨¡å‹ï¼ˆmax_depth=20ï¼‰
plot_learning_curves(
    DecisionTreeRegressor(max_depth=20, random_state=42),
    X, y, 
    'è¿‡æ‹Ÿåˆæ¨¡å‹çš„å­¦ä¹ æ›²çº¿'
)
```

**å­¦ä¹ æ›²çº¿åˆ†æ**ï¼š

```
ğŸ“ˆ æ¬ æ‹Ÿåˆçš„å­¦ä¹ æ›²çº¿
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è®­ç»ƒè¯¯å·® â”€â”€â”€â”€â”€â”€â”€ (é«˜ï¼Œå¹³ç¨³) â”‚
â”‚ éªŒè¯è¯¯å·® â”€â”€â”€â”€â”€â”€â”€ (é«˜ï¼Œå¹³ç¨³) â”‚
â”‚ ä¸¤æ¡çº¿è·ç¦»è¿‘ï¼Œéƒ½åœ¨é«˜ä½     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ç‰¹å¾ï¼šå¢åŠ æ•°æ®ä¹Ÿæ— æ³•æ”¹å–„

ğŸ“Š è‰¯å¥½æ‹Ÿåˆçš„å­¦ä¹ æ›²çº¿
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è®­ç»ƒè¯¯å·® â”€â”€â”€â”€â”€â”€â”€ (ä½ï¼Œä¸Šå‡) â”‚
â”‚ éªŒè¯è¯¯å·® â”€â”€â”€â”€â”€â”€â”€ (ä¸­ï¼Œä¸‹é™) â”‚
â”‚ ä¸¤æ¡çº¿é€æ¸æ¥è¿‘ï¼Œæ”¶æ•›åˆ°åˆç†å€¼â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ç‰¹å¾ï¼šè®­ç»ƒå’ŒéªŒè¯è¯¯å·®éƒ½è¾ƒä½

ğŸ“‰ è¿‡æ‹Ÿåˆçš„å­¦ä¹ æ›²çº¿
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è®­ç»ƒè¯¯å·® â”€â”€â”€â”€â”€â”€â”€ (å¾ˆä½)     â”‚
â”‚ éªŒè¯è¯¯å·® â”€â”€â”€â”€â”€â”€â”€ (é«˜)       â”‚
â”‚ ä¸¤æ¡çº¿ä¹‹é—´æœ‰å¾ˆå¤§çš„gap       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ç‰¹å¾ï¼šè®­ç»ƒè¯¯å·®å¾ˆä½ä½†éªŒè¯è¯¯å·®é«˜
```

---

### æ–¹æ³•äºŒï¼šäº¤å‰éªŒè¯å¯¹æ¯”

```python
from sklearn.model_selection import cross_val_score

def evaluate_model(model, X, y, model_name):
    """
    ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹
    """
    # è®­ç»ƒé›†å¾—åˆ†
    train_score = model.fit(X, y).score(X, y)
    
    # äº¤å‰éªŒè¯å¾—åˆ†
    cv_scores = cross_val_score(model, X, y, cv=5, 
                                scoring='r2')
    cv_mean = cv_scores.mean()
    cv_std = cv_scores.std()
    
    print(f"\n{model_name}:")
    print(f"  è®­ç»ƒé›† RÂ² åˆ†æ•°: {train_score:.4f}")
    print(f"  äº¤å‰éªŒè¯ RÂ² åˆ†æ•°: {cv_mean:.4f} (+/- {cv_std:.4f})")
    print(f"  è¿‡æ‹Ÿåˆç¨‹åº¦: {train_score - cv_mean:.4f}")
    
    # è¯Šæ–­
    if train_score - cv_mean > 0.1:
        print(f"  âš ï¸  è¯Šæ–­: è¿‡æ‹Ÿåˆ")
    elif train_score < 0.7 and cv_mean < 0.7:
        print(f"  âš ï¸  è¯Šæ–­: æ¬ æ‹Ÿåˆ")
    else:
        print(f"  âœ… è¯Šæ–­: æ‹Ÿåˆè‰¯å¥½")
    
    return train_score, cv_mean

# æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
from sklearn.tree import DecisionTreeRegressor

models = [
    (DecisionTreeRegressor(max_depth=1, random_state=42), "æ¬ æ‹Ÿåˆæ¨¡å‹"),
    (DecisionTreeRegressor(max_depth=5, random_state=42), "è‰¯å¥½æ¨¡å‹"),
    (DecisionTreeRegressor(max_depth=20, random_state=42), "è¿‡æ‹Ÿåˆæ¨¡å‹")
]

for model, name in models:
    evaluate_model(model, X, y, name)
```

---

## ğŸ›¡ï¸ é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•

### 1. å¢åŠ è®­ç»ƒæ•°æ®

**åŸç†**ï¼šæ›´å¤šæ•°æ® â†’ æ›´éš¾è®°ä½æ‰€æœ‰ç»†èŠ‚ â†’ è¢«è¿«å­¦ä¹ é€šç”¨è§„å¾‹

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

# ç”Ÿæˆæ•°æ®
def compare_data_sizes():
    """
    å¯¹æ¯”ä¸åŒæ•°æ®é‡å¯¹è¿‡æ‹Ÿåˆçš„å½±å“
    """
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    data_sizes = [50, 200, 1000]
    
    for ax, n_samples in zip(axes, data_sizes):
        # ç”Ÿæˆæ•°æ®
        X, y = make_moons(n_samples=n_samples, noise=0.3, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.3, random_state=42
        )
        
        # è®­ç»ƒæ¨¡å‹
        model = DecisionTreeClassifier(max_depth=10, random_state=42)
        model.fit(X_train, y_train)
        
        # è¯„ä¼°
        train_score = model.score(X_train, y_train)
        test_score = model.score(X_test, y_test)
        
        # ç»˜åˆ¶å†³ç­–è¾¹ç•Œ
        from matplotlib.colors import ListedColormap
        cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])
        cmap_bold = ListedColormap(['#FF0000', '#0000FF'])
        
        h = 0.02
        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                             np.arange(y_min, y_max, h))
        Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        ax.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.4)
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, 
                  cmap=cmap_bold, edgecolor='k', s=20, alpha=0.6)
        ax.set_title(f'æ•°æ®é‡: {n_samples}\nè®­ç»ƒ: {train_score:.2f}, æµ‹è¯•: {test_score:.2f}')
        ax.set_xlabel('ç‰¹å¾ 1')
        ax.set_ylabel('ç‰¹å¾ 2')
    
    plt.tight_layout()
    plt.show()

compare_data_sizes()
```

**å»ºè®®**ï¼š
- âœ… æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®
- âœ… æ•°æ®å¢å¼ºï¼ˆå›¾åƒæ—‹è½¬ã€ç¿»è½¬ç­‰ï¼‰
- âœ… åˆæˆæ•°æ®ï¼ˆSMOTEç­‰æŠ€æœ¯ï¼‰

---

### 2. é™ä½æ¨¡å‹å¤æ‚åº¦

**åŸç†**ï¼šç®€å•æ¨¡å‹ â†’ ä¸å®¹æ˜“è®°ä½ç»†èŠ‚ â†’ åªèƒ½å­¦é€šç”¨è§„å¾‹

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

# å¯¹æ¯”ä¸åŒå¤æ‚åº¦çš„å†³ç­–æ ‘
depths = range(1, 20)
train_scores = []
cv_scores = []

X, y = make_moons(n_samples=200, noise=0.3, random_state=42)

for depth in depths:
    model = DecisionTreeClassifier(max_depth=depth, random_state=42)
    
    # è®­ç»ƒåˆ†æ•°
    model.fit(X, y)
    train_scores.append(model.score(X, y))
    
    # äº¤å‰éªŒè¯åˆ†æ•°
    cv_score = cross_val_score(model, X, y, cv=5).mean()
    cv_scores.append(cv_score)

# ç»˜å›¾
plt.figure(figsize=(10, 6))
plt.plot(depths, train_scores, 'o-', label='è®­ç»ƒé›†å‡†ç¡®ç‡', color='blue')
plt.plot(depths, cv_scores, 'o-', label='äº¤å‰éªŒè¯å‡†ç¡®ç‡', color='red')
plt.xlabel('æ ‘çš„æ·±åº¦ï¼ˆæ¨¡å‹å¤æ‚åº¦ï¼‰', fontsize=12)
plt.ylabel('å‡†ç¡®ç‡', fontsize=12)
plt.title('æ¨¡å‹å¤æ‚åº¦ vs æ€§èƒ½', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.axvline(x=depths[np.argmax(cv_scores)], 
            color='green', linestyle='--', 
            label=f'æœ€ä½³æ·±åº¦: {depths[np.argmax(cv_scores)]}')
plt.legend()
plt.show()

print(f"æœ€ä½³æ ‘æ·±åº¦: {depths[np.argmax(cv_scores)]}")
print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {max(cv_scores):.4f}")
```

**å¸¸ç”¨æ–¹æ³•**ï¼š
- ğŸ”¹ å‡å°‘ç‰¹å¾æ•°é‡
- ğŸ”¹ é™ä½å¤šé¡¹å¼é˜¶æ•°
- ğŸ”¹ é™åˆ¶å†³ç­–æ ‘æ·±åº¦
- ğŸ”¹ å‡å°‘ç¥ç»ç½‘ç»œå±‚æ•°/èŠ‚ç‚¹æ•°

---

### 3. æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰

**åŸç†**ï¼šç»™æ¨¡å‹å¤æ‚åº¦åŠ "ç½šæ¬¾" â†’ è¿«ä½¿æ¨¡å‹ä¿æŒç®€å•

#### L1æ­£åˆ™åŒ–ï¼ˆLassoï¼‰

```python
from sklearn.linear_model import Lasso, Ridge, LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import Pipeline
import numpy as np

# ç”Ÿæˆæ•°æ®
np.random.seed(42)
X = np.linspace(0, 10, 30).reshape(-1, 1)
y = 3*X.ravel() + 5 + np.random.randn(30) * 5

# åˆ›å»ºé«˜é˜¶ç‰¹å¾ï¼ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼‰
poly = PolynomialFeatures(degree=10)
X_poly = poly.fit_transform(X)

# å¯¹æ¯”ä¸åŒæ­£åˆ™åŒ–
fig, axes = plt.subplots(1, 3, figsize=(15, 4))
X_test = np.linspace(0, 10, 100).reshape(-1, 1)
X_test_poly = poly.transform(X_test)

models = [
    ('æ— æ­£åˆ™åŒ–', LinearRegression()),
    ('L1æ­£åˆ™åŒ– (Lasso)', Lasso(alpha=1.0)),
    ('L2æ­£åˆ™åŒ– (Ridge)', Ridge(alpha=1.0))
]

for ax, (name, model) in zip(axes, models):
    model.fit(X_poly, y)
    y_pred = model.predict(X_test_poly)
    
    ax.scatter(X, y, alpha=0.6, label='è®­ç»ƒæ•°æ®')
    ax.plot(X_test, y_pred, 'r-', linewidth=2, label='æ¨¡å‹é¢„æµ‹')
    ax.set_title(name, fontsize=13, fontweight='bold')
    ax.set_xlabel('X')
    ax.set_ylabel('y')
    ax.legend()
    ax.grid(True, alpha=0.3)
    ax.set_ylim(-10, 50)

plt.tight_layout()
plt.show()

# æ‰“å°ç³»æ•°
print("\næ¨¡å‹ç³»æ•°å¯¹æ¯”:")
for name, model in models:
    model.fit(X_poly, y)
    non_zero = np.sum(np.abs(model.coef_) > 0.01)
    print(f"{name}: {non_zero} ä¸ªéé›¶ç³»æ•°")
```

**æ­£åˆ™åŒ–ç±»å‹**ï¼š

| ç±»å‹ | å…¬å¼ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **L1 (Lasso)** | æŸå¤± + Î±âˆ‘\|w\| | äº§ç”Ÿç¨€ç–è§£<br/>ç‰¹å¾é€‰æ‹© | ç‰¹å¾å¾ˆå¤šï¼Œéœ€è¦è‡ªåŠ¨ç­›é€‰ |
| **L2 (Ridge)** | æŸå¤± + Î±âˆ‘wÂ² | æƒé‡è¡°å‡<br/>ä¿ç•™æ‰€æœ‰ç‰¹å¾ | ç‰¹å¾ç›¸å…³æ€§é«˜ |
| **Elastic Net** | L1 + L2ç»„åˆ | ç»“åˆä¸¤è€…ä¼˜ç‚¹ | ç‰¹å¾å¤šä¸”ç›¸å…³ |

---

### 4. æ—©åœï¼ˆEarly Stoppingï¼‰

**åŸç†**ï¼šåœ¨éªŒè¯é›†æ€§èƒ½å¼€å§‹ä¸‹é™æ—¶åœæ­¢è®­ç»ƒ

```python
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
X, y = make_moons(n_samples=500, noise=0.3, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# è®­ç»ƒæ¨¡å‹å¹¶è®°å½•å†å²
model = MLPRegressor(
    hidden_layer_sizes=(100, 50),
    max_iter=1000,
    early_stopping=True,
    validation_fraction=0.2,
    random_state=42,
    verbose=False
)

model.fit(X_train, y_train)

# ç»˜åˆ¶è®­ç»ƒæ›²çº¿
plt.figure(figsize=(10, 6))
plt.plot(model.loss_curve_, label='è®­ç»ƒæŸå¤±', linewidth=2)
plt.xlabel('è¿­ä»£æ¬¡æ•°', fontsize=12)
plt.ylabel('æŸå¤±', fontsize=12)
plt.title('æ—©åœæ³•ç¤ºä¾‹', fontsize=14, fontweight='bold')
plt.axvline(x=model.n_iter_, color='red', linestyle='--', 
            label=f'æ—©åœç‚¹ (è¿­ä»£{model.n_iter_}æ¬¡)')
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.show()

print(f"æ—©åœåœ¨ç¬¬ {model.n_iter_} æ¬¡è¿­ä»£")
```

**æ—©åœç­–ç•¥**ï¼š

![æ—©åœç­–ç•¥æµç¨‹å›¾](/content/images/lesson-05-æ—©åœç­–ç•¥.png)

---

### 5. Dropoutï¼ˆç¥ç»ç½‘ç»œä¸“ç”¨ï¼‰

**åŸç†**ï¼šè®­ç»ƒæ—¶éšæœº"å…³é—­"ä¸€äº›ç¥ç»å…ƒ â†’ é˜²æ­¢è¿‡åº¦ä¾èµ–æŸäº›ç‰¹å¾

```python
from tensorflow import keras
from tensorflow.keras import layers

# ä¸ä½¿ç”¨Dropout
model_no_dropout = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(10,)),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])

# ä½¿ç”¨Dropout
model_with_dropout = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(10,)),
    layers.Dropout(0.5),  # éšæœºå…³é—­50%çš„ç¥ç»å…ƒ
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.3),  # éšæœºå…³é—­30%çš„ç¥ç»å…ƒ
    layers.Dense(1)
])

print("æ¨¡å‹ç»“æ„:")
model_with_dropout.summary()
```

**Dropoutæ¯”å–»**ï¼š
```
æƒ³è±¡ä¸€ä¸ªå›¢é˜Ÿé¡¹ç›®ï¼š

âŒ æ²¡æœ‰Dropout:
   æ¯æ¬¡éƒ½æ˜¯åŒæ ·çš„äººåšåŒæ ·çš„å·¥ä½œ
   â†’ å½¢æˆå›ºå®šå¥—è·¯
   â†’ ç¼ºä¹çµæ´»æ€§

âœ… æœ‰Dropout:
   æ¯æ¬¡éšæœºæœ‰äº›äººä¸å‚ä¸
   â†’ å…¶ä»–äººå¿…é¡»å­¦ä¼šä»–ä»¬çš„å·¥ä½œ
   â†’ æ•´ä¸ªå›¢é˜Ÿæ›´åŠ çµæ´»robust
```

---

### 6. æ•°æ®å¢å¼ºï¼ˆData Augmentationï¼‰

**åŸç†**ï¼šé€šè¿‡å˜æ¢å¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§

```python
# å›¾åƒæ•°æ®å¢å¼ºç¤ºä¾‹
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,      # éšæœºæ—‹è½¬20åº¦
    width_shift_range=0.2,  # æ°´å¹³å¹³ç§»
    height_shift_range=0.2, # å‚ç›´å¹³ç§»
    horizontal_flip=True,   # æ°´å¹³ç¿»è½¬
    zoom_range=0.2,         # éšæœºç¼©æ”¾
    fill_mode='nearest'     # å¡«å……æ¨¡å¼
)

# æ–‡æœ¬æ•°æ®å¢å¼ºç¤ºä¾‹
def augment_text(text):
    """
    æ–‡æœ¬æ•°æ®å¢å¼º
    """
    augmented = []
    
    # 1. åŒä¹‰è¯æ›¿æ¢
    # 2. éšæœºæ’å…¥
    # 3. éšæœºåˆ é™¤
    # 4. éšæœºäº¤æ¢
    
    return augmented

# è¡¨æ ¼æ•°æ®å¢å¼ºç¤ºä¾‹
def augment_tabular(X):
    """
    è¡¨æ ¼æ•°æ®å¢å¼º
    """
    # 1. æ·»åŠ å™ªå£°
    noise = np.random.normal(0, 0.01, X.shape)
    X_augmented = X + noise
    
    # 2. SMOTEï¼ˆé’ˆå¯¹ä¸å¹³è¡¡æ•°æ®ï¼‰
    from imblearn.over_sampling import SMOTE
    smote = SMOTE(random_state=42)
    X_resampled, y_resampled = smote.fit_resample(X, y)
    
    return X_augmented
```

---

## ğŸ¯ è§£å†³æ¬ æ‹Ÿåˆçš„æ–¹æ³•

### 1. å¢åŠ æ¨¡å‹å¤æ‚åº¦

```python
# ä»ç®€å•åˆ°å¤æ‚
models = [
    ('çº¿æ€§å›å½’', LinearRegression()),
    ('2é˜¶å¤šé¡¹å¼', Pipeline([
        ('poly', PolynomialFeatures(2)),
        ('linear', LinearRegression())
    ])),
    ('5é˜¶å¤šé¡¹å¼', Pipeline([
        ('poly', PolynomialFeatures(5)),
        ('linear', LinearRegression())
    ])),
    ('å†³ç­–æ ‘', DecisionTreeRegressor(max_depth=5))
]

for name, model in models:
    model.fit(X_train, y_train)
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    print(f"{name:15s}: è®­ç»ƒ={train_score:.3f}, æµ‹è¯•={test_score:.3f}")
```

### 2. å¢åŠ ç‰¹å¾

```python
from sklearn.preprocessing import PolynomialFeatures

# åŸå§‹ç‰¹å¾
print(f"åŸå§‹ç‰¹å¾æ•°: {X.shape[1]}")

# æ·»åŠ å¤šé¡¹å¼ç‰¹å¾
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)
print(f"æ‰©å±•åç‰¹å¾æ•°: {X_poly.shape[1]}")

# æ·»åŠ äº¤äº’ç‰¹å¾
from sklearn.preprocessing import PolynomialFeatures
poly_interaction = PolynomialFeatures(
    degree=2, 
    interaction_only=True,
    include_bias=False
)
X_interaction = poly_interaction.fit_transform(X)
print(f"äº¤äº’ç‰¹å¾æ•°: {X_interaction.shape[1]}")
```

### 3. å‡å°‘æ­£åˆ™åŒ–å¼ºåº¦

```python
# å¯¹æ¯”ä¸åŒæ­£åˆ™åŒ–å¼ºåº¦
alphas = [100, 10, 1, 0.1, 0.01]

for alpha in alphas:
    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    print(f"Alpha={alpha:6.2f}: è®­ç»ƒ={train_score:.3f}, æµ‹è¯•={test_score:.3f}")
```

### 4. è®­ç»ƒæ›´é•¿æ—¶é—´

```python
from sklearn.neural_network import MLPRegressor

# çŸ­æ—¶é—´è®­ç»ƒï¼ˆå¯èƒ½æ¬ æ‹Ÿåˆï¼‰
model_short = MLPRegressor(max_iter=10, random_state=42)
model_short.fit(X_train, y_train)

# é•¿æ—¶é—´è®­ç»ƒ
model_long = MLPRegressor(max_iter=1000, random_state=42)
model_long.fit(X_train, y_train)

print(f"çŸ­è®­ç»ƒ - è®­ç»ƒåˆ†æ•°: {model_short.score(X_train, y_train):.3f}")
print(f"é•¿è®­ç»ƒ - è®­ç»ƒåˆ†æ•°: {model_long.score(X_train, y_train):.3f}")
```

---

## âš–ï¸ åå·®-æ–¹å·®æƒè¡¡

### æ ¸å¿ƒæ¦‚å¿µ

![åå·®-æ–¹å·®æƒè¡¡å›¾](/content/images/lesson-05-åå·®-æ–¹å·®æƒè¡¡.png)

### å®šä¹‰

**åå·®ï¼ˆBiasï¼‰**ï¼š
- æ¨¡å‹é¢„æµ‹çš„æœŸæœ›å€¼ä¸çœŸå®å€¼çš„å·®è·
- é«˜åå·® = æ¨¡å‹å¤ªç®€å• = æ¬ æ‹Ÿåˆ
- åæ˜ æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›

**æ–¹å·®ï¼ˆVarianceï¼‰**ï¼š
- æ¨¡å‹åœ¨ä¸åŒè®­ç»ƒé›†ä¸Šé¢„æµ‹çš„å˜åŒ–ç¨‹åº¦
- é«˜æ–¹å·® = æ¨¡å‹å¤ªå¤æ‚ = è¿‡æ‹Ÿåˆ
- åæ˜ æ¨¡å‹çš„ç¨³å®šæ€§

**ä¸å¯çº¦è¯¯å·®ï¼ˆIrreducible Errorï¼‰**ï¼š
- æ•°æ®æœ¬èº«çš„å™ªå£°
- æ— æ³•é€šè¿‡æ¨¡å‹æ”¹è¿›æ¶ˆé™¤

### æ•°å­¦è¡¨è¾¾

```
æ€»è¯¯å·® = åå·®Â² + æ–¹å·® + ä¸å¯çº¦è¯¯å·®

E[(y - Å·)Â²] = Bias[Å·]Â² + Var[Å·] + ÏƒÂ²
```

### å¯è§†åŒ–ç†è§£

```
ğŸ¯ å°„å‡»ç›®æ ‡æ¯”å–»ï¼š

ä½åå·®ï¼Œä½æ–¹å·®ï¼ˆç†æƒ³ï¼‰        é«˜åå·®ï¼Œä½æ–¹å·®ï¼ˆæ¬ æ‹Ÿåˆï¼‰
    â—â—â—                           â—‹ â—‹
     â—â—                          â—‹   â—‹
    â—â—â—                           â—‹ â—‹
   ï¼ˆéƒ½åœ¨é¶å¿ƒï¼‰                  ï¼ˆåç¦»é¶å¿ƒä½†é›†ä¸­ï¼‰

ä½åå·®ï¼Œé«˜æ–¹å·®ï¼ˆè¿‡æ‹Ÿåˆï¼‰      é«˜åå·®ï¼Œé«˜æ–¹å·®ï¼ˆæœ€å·®ï¼‰
  â—     â—                        â—‹       â—‹
      â—                                â—‹
 â—       â—                       â—‹         â—‹
ï¼ˆæ¥è¿‘é¶å¿ƒä½†åˆ†æ•£ï¼‰              ï¼ˆåç¦»ä¸”åˆ†æ•£ï¼‰
```

### æƒè¡¡ç­–ç•¥

```python
def bias_variance_decomposition(model, X, y, n_iterations=100):
    """
    åå·®-æ–¹å·®åˆ†è§£
    """
    from sklearn.model_selection import train_test_split
    
    predictions = []
    
    for i in range(n_iterations):
        # éšæœºé‡‡æ ·
        X_sample, _, y_sample, _ = train_test_split(
            X, y, test_size=0.3, random_state=i
        )
        
        # è®­ç»ƒæ¨¡å‹
        model.fit(X_sample, y_sample)
        
        # é¢„æµ‹
        y_pred = model.predict(X)
        predictions.append(y_pred)
    
    predictions = np.array(predictions)
    
    # è®¡ç®—åå·®å’Œæ–¹å·®
    bias = np.mean((predictions.mean(axis=0) - y) ** 2)
    variance = np.mean(np.var(predictions, axis=0))
    
    print(f"åå·®Â²: {bias:.4f}")
    print(f"æ–¹å·®:  {variance:.4f}")
    print(f"æ€»å’Œ:  {bias + variance:.4f}")
    
    return bias, variance

# æµ‹è¯•ä¸åŒå¤æ‚åº¦çš„æ¨¡å‹
from sklearn.tree import DecisionTreeRegressor

print("ç®€å•æ¨¡å‹ï¼ˆæ¬ æ‹Ÿåˆï¼‰:")
bias_variance_decomposition(
    DecisionTreeRegressor(max_depth=1, random_state=42),
    X, y
)

print("\né€‚ä¸­æ¨¡å‹:")
bias_variance_decomposition(
    DecisionTreeRegressor(max_depth=5, random_state=42),
    X, y
)

print("\nå¤æ‚æ¨¡å‹ï¼ˆè¿‡æ‹Ÿåˆï¼‰:")
bias_variance_decomposition(
    DecisionTreeRegressor(max_depth=20, random_state=42),
    X, y
)
```

---

## ğŸ“‹ å®æˆ˜å†³ç­–æµç¨‹

![å®æˆ˜å†³ç­–æµç¨‹å›¾](/content/images/lesson-05-å®æˆ˜å†³ç­–æµç¨‹.png)

---

## ğŸ“ å®æˆ˜æ¡ˆä¾‹ï¼šæˆ¿ä»·é¢„æµ‹

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
import numpy as np

# 1. åŠ è½½æ•°æ®
housing = fetch_california_housing()
X, y = housing.data, housing.target

# 2. åˆ’åˆ†æ•°æ®
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 3. æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. æµ‹è¯•ä¸åŒæ¨¡å‹
models = {
    'çº¿æ€§å›å½’ï¼ˆå¯èƒ½æ¬ æ‹Ÿåˆï¼‰': Ridge(alpha=100),
    'Ridgeå›å½’ï¼ˆå¹³è¡¡ï¼‰': Ridge(alpha=1.0),
    'éšæœºæ£®æ—ï¼ˆå¯èƒ½è¿‡æ‹Ÿåˆï¼‰': RandomForestRegressor(
        n_estimators=100, 
        max_depth=None,  # æ— é™åˆ¶
        random_state=42
    ),
    'éšæœºæ£®æ—ï¼ˆæ­£åˆ™åŒ–ï¼‰': RandomForestRegressor(
        n_estimators=100, 
        max_depth=10,  # é™åˆ¶æ·±åº¦
        min_samples_split=20,  # æœ€å°åˆ†è£‚æ ·æœ¬æ•°
        random_state=42
    )
}

print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”:\n")
print(f"{'æ¨¡å‹':<25s} {'è®­ç»ƒRÂ²':<12s} {'æµ‹è¯•RÂ²':<12s} {'è¿‡æ‹Ÿåˆç¨‹åº¦':<12s}")
print("-" * 60)

for name, model in models.items():
    # è®­ç»ƒ
    model.fit(X_train_scaled, y_train)
    
    # è¯„ä¼°
    train_score = model.score(X_train_scaled, y_train)
    test_score = model.score(X_test_scaled, y_test)
    overfit = train_score - test_score
    
    # è¾“å‡º
    status = "âœ…" if overfit < 0.05 else "âš ï¸"
    print(f"{name:<25s} {train_score:<12.4f} {test_score:<12.4f} {overfit:<12.4f} {status}")

# 5. ä½¿ç”¨äº¤å‰éªŒè¯æ‰¾æœ€ä½³å‚æ•°
print("\n\næ­£åœ¨æœç´¢æœ€ä½³å‚æ•°...")

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [5, 10, 15, 20],
    'min_samples_split': [2, 10, 20],
    'min_samples_leaf': [1, 5, 10]
}

grid_search = GridSearchCV(
    RandomForestRegressor(n_estimators=100, random_state=42),
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1
)

grid_search.fit(X_train_scaled, y_train)

print(f"\næœ€ä½³å‚æ•°: {grid_search.best_params_}")
print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.4f}")

# æœ€ç»ˆæ¨¡å‹è¯„ä¼°
best_model = grid_search.best_estimator_
train_score = best_model.score(X_train_scaled, y_train)
test_score = best_model.score(X_test_scaled, y_test)

print(f"\næœ€ç»ˆæ¨¡å‹æ€§èƒ½:")
print(f"è®­ç»ƒé›† RÂ²: {train_score:.4f}")
print(f"æµ‹è¯•é›† RÂ²: {test_score:.4f}")
print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {train_score - test_score:.4f}")

if train_score - test_score < 0.03:
    print("âœ… æ¨¡å‹æ‹Ÿåˆè‰¯å¥½ï¼")
elif train_score - test_score > 0.1:
    print("âš ï¸ æ¨¡å‹å­˜åœ¨è¿‡æ‹Ÿåˆ")
else:
    print("âš¡ æ¨¡å‹å¯ä»¥æ¥å—ï¼Œä½†è¿˜æœ‰æ”¹è¿›ç©ºé—´")
```

---

## ğŸ¯ å­¦ä¹ è¦ç‚¹æ€»ç»“

### æ ¸å¿ƒæ¦‚å¿µå¯¹æ¯”

| ç»´åº¦ | æ¬ æ‹Ÿåˆ | åˆšåˆšå¥½ | è¿‡æ‹Ÿåˆ |
|------|--------|--------|--------|
| **è®­ç»ƒé›†è¡¨ç°** | å·® | å¥½ | å¾ˆå¥½ |
| **æµ‹è¯•é›†è¡¨ç°** | å·® | å¥½ | å·® |
| **æ¨¡å‹å¤æ‚åº¦** | å¤ªä½ | é€‚ä¸­ | å¤ªé«˜ |
| **åå·®** | é«˜ | ä¸­ | ä½ |
| **æ–¹å·®** | ä½ | ä¸­ | é«˜ |
| **æ³›åŒ–èƒ½åŠ›** | å·® | å¥½ | å·® |

### è¯Šæ–­æ–¹æ³•

```
âœ… å¦‚ä½•è¯Šæ–­ï¼Ÿ

1. å­¦ä¹ æ›²çº¿
   - è§‚å¯Ÿè®­ç»ƒå’ŒéªŒè¯è¯¯å·®çš„gap
   
2. äº¤å‰éªŒè¯
   - å¯¹æ¯”è®­ç»ƒåˆ†æ•°å’ŒCVåˆ†æ•°
   
3. å¯è§†åŒ–
   - ç»˜åˆ¶æ¨¡å‹é¢„æµ‹æ›²çº¿
   
4. æŒ‡æ ‡å¯¹æ¯”
   - è®­ç»ƒé›† vs æµ‹è¯•é›†æ€§èƒ½
```

### è§£å†³æ–¹æ¡ˆé€ŸæŸ¥

```
ğŸ”§ è§£å†³æ¬ æ‹Ÿåˆï¼š
âœ“ å¢åŠ æ¨¡å‹å¤æ‚åº¦
âœ“ å¢åŠ ç‰¹å¾
âœ“ å‡å°‘æ­£åˆ™åŒ–
âœ“ è®­ç»ƒæ›´é•¿æ—¶é—´

ğŸ”§ è§£å†³è¿‡æ‹Ÿåˆï¼š
âœ“ å¢åŠ è®­ç»ƒæ•°æ®
âœ“ é™ä½æ¨¡å‹å¤æ‚åº¦
âœ“ æ­£åˆ™åŒ–ï¼ˆL1/L2ï¼‰
âœ“ Dropout
âœ“ æ—©åœ
âœ“ æ•°æ®å¢å¼º
âœ“ é›†æˆæ–¹æ³•
```

---

## ğŸ’¡ æœ€ä½³å®è·µå»ºè®®

### 1. ä»ç®€å•å¼€å§‹

```python
# âœ… æ¨èæµç¨‹
# ç¬¬ä¸€æ­¥ï¼šç®€å•æ¨¡å‹ï¼ˆåŸºçº¿ï¼‰
baseline = LinearRegression()

# ç¬¬äºŒæ­¥ï¼šé€‚åº¦å¤æ‚æ¨¡å‹
moderate = RandomForestRegressor(max_depth=5)

# ç¬¬ä¸‰æ­¥ï¼šå¤æ‚æ¨¡å‹ï¼ˆå¦‚éœ€è¦ï¼‰
complex = RandomForestRegressor(max_depth=None)

# å¯¹æ¯”æ€§èƒ½ï¼Œé€‰æ‹©æœ€ä½³
```

### 2. ä½¿ç”¨äº¤å‰éªŒè¯

```python
from sklearn.model_selection import cross_val_score

# âœ… æ€»æ˜¯ä½¿ç”¨äº¤å‰éªŒè¯
scores = cross_val_score(model, X, y, cv=5)
print(f"CVå‡å€¼: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

### 3. ç›‘æ§è®­ç»ƒè¿‡ç¨‹

```python
from sklearn.model_selection import learning_curve

# âœ… ç»˜åˆ¶å­¦ä¹ æ›²çº¿
train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, cv=5, n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)
# ç»˜å›¾è§‚å¯Ÿ...
```

### 4. ä¿å­˜æœ€ä½³æ¨¡å‹

```python
import joblib

# âœ… ä¿å­˜æ¨¡å‹
joblib.dump(best_model, 'best_model.pkl')

# åŠ è½½æ¨¡å‹
loaded_model = joblib.load('best_model.pkl')
```

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

### æ¨èèµ„æº

**ç†è®ºåŸºç¡€**ï¼š
- ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹- æèˆª
- ã€Šæœºå™¨å­¦ä¹ ã€‹- å‘¨å¿—åï¼ˆè¥¿ç“œä¹¦ï¼‰
- "Understanding the Bias-Variance Tradeoff" - Scott Fortmann-Roe

**å®è·µæŠ€å·§**ï¼š
- Scikit-learn: Model Evaluation
- Kaggle: Overfitting and How to Prevent It
- Andrew Ng: Machine Learning Course (Coursera)

**é«˜çº§è¯é¢˜**ï¼š
- æ­£åˆ™åŒ–ç†è®º
- é›†æˆå­¦ä¹ 
- æ·±åº¦å­¦ä¹ ä¸­çš„æ­£åˆ™åŒ–æŠ€æœ¯

### æ·±å…¥è¯é¢˜

1. **æ­£åˆ™åŒ–è¿›é˜¶**
   - Elastic Net
   - Group Lasso
   - Dropoutå˜ç§

2. **æ¨¡å‹é€‰æ‹©**
   - AIC/BICå‡†åˆ™
   - MDLåŸç†
   - ç»“æ„é£é™©æœ€å°åŒ–

3. **é›†æˆæ–¹æ³•**
   - Bagging (å‡å°‘æ–¹å·®)
   - Boosting (å‡å°‘åå·®)
   - Stacking

---

## âœ… ä¸‹èŠ‚é¢„å‘Š

**ç¬¬6è¯¾ï¼šæ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼ˆåˆ†ç±»ï¼‰**

ä¸‹ä¸€è¯¾æˆ‘ä»¬å°†å­¦ä¹ ï¼š
- å‡†ç¡®ç‡ã€ç²¾ç¡®ç‡ã€å¬å›ç‡
- F1åˆ†æ•°å’Œæ··æ·†çŸ©é˜µ
- ROCæ›²çº¿å’ŒAUC
- å¦‚ä½•é€‰æ‹©åˆé€‚çš„è¯„ä¼°æŒ‡æ ‡

**é¢„ä¹ ä»»åŠ¡**ï¼š
- æ€è€ƒï¼šå‡†ç¡®ç‡ä¸ºä»€ä¹ˆä¸å¤Ÿç”¨ï¼Ÿ
- äº†è§£ï¼šä»€ä¹ˆæ˜¯æ··æ·†çŸ©é˜µï¼Ÿ
- å‡†å¤‡ï¼šä¸€ä¸ªåˆ†ç±»é—®é¢˜æ¡ˆä¾‹

---

**æ­å–œä½ å®Œæˆç¬¬5è¯¾ï¼** ğŸ‰

ç°åœ¨ä½ å·²ç»æŒæ¡äº†è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆçš„æ ¸å¿ƒçŸ¥è¯†ã€‚è®°ä½ï¼š
- ğŸ¯ **ä»ç®€å•æ¨¡å‹å¼€å§‹**
- ğŸ“Š **ç›‘æ§è®­ç»ƒå’ŒéªŒè¯æ€§èƒ½**
- âš–ï¸ **å¹³è¡¡åå·®å’Œæ–¹å·®**
- ğŸ”§ **é€‰æ‹©åˆé€‚çš„æ­£åˆ™åŒ–æ–¹æ³•**

ç»§ç»­åŠ æ²¹ï¼ä¸‹ä¸€è¯¾è§ï¼ğŸ’ª

